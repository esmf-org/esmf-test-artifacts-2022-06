build time -- 2022-03-13 04:08:46
20220313 044934.387 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220313 044934.387 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220313 044934.387 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220313 044934.387 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220313 044934.387 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET0 Running with ESMF Version   : v8.3.0b09
20220313 044934.387 INFO             PET0 ESMF library build date/time: "Mar 13 2022" "04:05:42"
20220313 044934.387 INFO             PET0 ESMF library build location : /mnt/lfs4/HFIP/hfv3gfs/Mark.Potts/intel_2020.2_mvapich2_g_develop
20220313 044934.387 INFO             PET0 ESMF_COMM                   : mvapich2
20220313 044934.388 INFO             PET0 ESMF_MOAB                   : enabled
20220313 044934.388 INFO             PET0 ESMF_LAPACK                 : enabled
20220313 044934.388 INFO             PET0 ESMF_NETCDF                 : enabled
20220313 044934.388 INFO             PET0 ESMF_PNETCDF                : disabled
20220313 044934.388 INFO             PET0 ESMF_PIO                    : enabled
20220313 044934.388 INFO             PET0 ESMF_YAMLCPP                : enabled
20220313 044934.389 INFO             PET0 --- VMK::logSystem() start -------------------------------
20220313 044934.389 INFO             PET0 esmfComm=mvapich2
20220313 044934.389 INFO             PET0 isPthreadsEnabled=0
20220313 044934.389 INFO             PET0 isOpenMPEnabled=1
20220313 044934.389 INFO             PET0 isOpenACCEnabled=0
20220313 044934.389 INFO             PET0 isSsiSharedMemoryEnabled=1
20220313 044934.389 INFO             PET0 ssiCount=1 peCount=6
20220313 044934.389 INFO             PET0 PE=0 SSI=0 SSIPE=0
20220313 044934.389 INFO             PET0 PE=1 SSI=0 SSIPE=1
20220313 044934.389 INFO             PET0 PE=2 SSI=0 SSIPE=2
20220313 044934.389 INFO             PET0 PE=3 SSI=0 SSIPE=3
20220313 044934.389 INFO             PET0 PE=4 SSI=0 SSIPE=4
20220313 044934.389 INFO             PET0 PE=5 SSI=0 SSIPE=5
20220313 044934.389 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20220313 044934.389 INFO             PET0 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220313 044934.389 INFO             PET0 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220313 044934.389 INFO             PET0 index=   2                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220313 044934.389 INFO             PET0 index=   3                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.389 INFO             PET0 index=   4                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220313 044934.389 INFO             PET0 index=   5                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220313 044934.389 INFO             PET0 index=   6                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220313 044934.389 INFO             PET0 index=   7                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=   8                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=   9                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20220313 044934.390 INFO             PET0 index=  10                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220313 044934.390 INFO             PET0 index=  11                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=  12                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=  13                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=  14                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=  15                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220313 044934.390 INFO             PET0 index=  16                       MPIR_CVAR_GATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for gather operation.
20220313 044934.390 INFO             PET0 index=  17                    MPIR_CVAR_ALLGATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allgather operation.
20220313 044934.390 INFO             PET0 index=  18                    MPIR_CVAR_ALLREDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allreduce operation.
20220313 044934.390 INFO             PET0 index=  19                     MPIR_CVAR_ALLTOALL_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoall operation.
20220313 044934.390 INFO             PET0 index=  20                    MPIR_CVAR_ALLTOALLV_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoallv operation.
20220313 044934.390 INFO             PET0 index=  21                        MPIR_CVAR_BCAST_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for broadcast operation.
20220313 044934.390 INFO             PET0 index=  22                       MPIR_CVAR_REDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for reduce operation.
20220313 044934.390 INFO             PET0 index=  23                      MPIR_CVAR_SCATTER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for scatter operation.
20220313 044934.390 INFO             PET0 index=  24                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=  25                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220313 044934.390 INFO             PET0 index=  26                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220313 044934.390 INFO             PET0 index=  27                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20220313 044934.390 INFO             PET0 index=  28                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.390 INFO             PET0 index=  29                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220313 044934.390 INFO             PET0 index=  30                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220313 044934.390 INFO             PET0 index=  31                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220313 044934.390 INFO             PET0 index=  32                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20220313 044934.390 INFO             PET0 index=  33                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20220313 044934.390 INFO             PET0 index=  34                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20220313 044934.390 INFO             PET0 index=  35                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPIR_WaitForDebugger-time.
20220313 044934.390 INFO             PET0 index=  36                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20220313 044934.390 INFO             PET0 index=  37                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20220313 044934.390 INFO             PET0 index=  38                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20220313 044934.390 INFO             PET0 index=  39                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20220313 044934.390 INFO             PET0 index=  40                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20220313 044934.390 INFO             PET0 index=  41                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20220313 044934.390 INFO             PET0 index=  42                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20220313 044934.391 INFO             PET0 index=  43                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20220313 044934.391 INFO             PET0 index=  44                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20220313 044934.391 INFO             PET0 index=  45                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20220313 044934.391 INFO             PET0 index=  46                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20220313 044934.391 INFO             PET0 index=  47                                   MPIR_CVAR_FORCE_ARCH_TYPE : This parameter forces the architecture type.
20220313 044934.391 INFO             PET0 index=  48                                    MPIR_CVAR_FORCE_HCA_TYPE : This parameter forces the HCA type.
20220313 044934.391 INFO             PET0 index=  49                                      MPIR_CVAR_USE_BLOCKING : Setting this parameter enables mvapich2 to use blocking mode progress. MPI applications do not take up any CPU when they are waiting for incoming messages.
20220313 044934.391 INFO             PET0 index=  50                                    MPIR_CVAR_USE_SHARED_MEM : Use shared memory for intra-node communication.
20220313 044934.391 INFO             PET0 index=  51                               MPIR_CVAR_ON_DEMAND_THRESHOLD : This defines threshold for enabling on-demand connection management scheme. When the size of the job is larger than the threshold value, on-demand connection management will be used.
20220313 044934.391 INFO             PET0 index=  52                                      MPIR_CVAR_ENABLE_SHARP : This enables the hardware-based SHArP collectives.
20220313 044934.391 INFO             PET0 index=  53                                     MPIR_CVAR_SM_SCHEDULING : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET0 index=  54                     MPIR_CVAR_SMALL_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with small message sizes.
20220313 044934.391 INFO             PET0 index=  55                       MPIR_CVAR_MED_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with medium message sizes.
20220313 044934.391 INFO             PET0 index=  56                               MPIR_CVAR_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET0 index=  57                                         MPIR_CVAR_NUM_PORTS : This specifies the number of ports per InfiniBand adapter to be used for communication per adapter on an end node.
20220313 044934.391 INFO             PET0 index=  58                                   MPIR_CVAR_NUM_QP_PER_PORT : This parameter indicates number of queue pairs per port to be used for communication on an end node. This is useful in the presence of multiple send/recv engines available per port for data transfer.
20220313 044934.391 INFO             PET0 index=  59                               MPIR_CVAR_IBA_EAGER_THRESHOLD : This specifies the switch point between eager and rendezvous protocol in MVAPICH2. For better performance, the value of MPIR_CVAR_MV2_IBA_EAGER_THRESHOLD should be set the same as MPIR_CVAR_MV2_VBUF_TOTAL_SIZE.
20220313 044934.391 INFO             PET0 index=  60                                MPIR_CVAR_STRIPING_THRESHOLD : This parameter specifies the message size above which we begin to stripe the message across multiple rails (if present).
20220313 044934.391 INFO             PET0 index=  61                    MPIR_CVAR_RAIL_SHARING_MED_MSG_THRESHOLD : This specifies the threshold for the medium message size beyond which medium rail sharing striping will take place.
20220313 044934.391 INFO             PET0 index=  62                  MPIR_CVAR_RAIL_SHARING_LARGE_MSG_THRESHOLD : This specifies the threshold for the large message size beyond which large rail sharing striping will be effective.
20220313 044934.391 INFO             PET0 index=  63                                         MPIR_CVAR_USE_MCAST : Set this to 1, to enable hardware multicast support in collective communication.
20220313 044934.391 INFO             PET0 index=  64                                MPIR_CVAR_COALESCE_THRESHOLD : This parameter determines the threshhold for message coalescing.
20220313 044934.391 INFO             PET0 index=  65                                      MPIR_CVAR_USE_COALESCE : Coalesce multiple small messages into a single message to increase small message throughput.
20220313 044934.391 INFO             PET0 index=  66                                     MPIR_CVAR_RNDV_PROTOCOL : The value of this variable can be set to choose different rendezvous protocols. RPUT (default RDMA-Write) RGET (RDMA Read based), R3 (send/recv based).
20220313 044934.391 INFO             PET0 index=  67                                        MPIR_CVAR_SPIN_COUNT : This is the number of the connection manager polls for new control messages from UD channel for each interrupt. This may be increased to reduce the interrupt overhead when many incoming control messages from  UD channel at the same time.
20220313 044934.391 INFO             PET0 index=  68                                       MPIR_CVAR_DEFAULT_MTU : The internal MTU size. For Gen2, this parameter should be a string instead of an integer. Valid values are: IBV_MTU_256, IBV_MTU_512, IBV_MTU_1024, IBV_MTU_2048, IBV_MTU_4096.
20220313 044934.391 INFO             PET0 index=  69                                 MPIR_CVAR_NUM_CQES_PER_POLL : Maximum number of InfiniBand messages retrieved from the completion queue in one attempt.
20220313 044934.391 INFO             PET0 index=  70                                       MPIR_CVAR_USE_RDMA_CM : This parameter enables the use of RDMA CM for establishing the connections.
20220313 044934.391 INFO             PET0 index=  71                                    MPIR_CVAR_USE_IWARP_MODE : This parameter enables the library to run in iWARP mode.
20220313 044934.391 INFO             PET0 index=  72                                       MPIR_CVAR_SUPPORT_DPM : This option enables the dynamic process management interface and on-demand connection management.
20220313 044934.391 INFO             PET0 index=  73                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20220313 044934.391 INFO             PET0 index=  74                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20220313 044934.391 INFO             PET0 index=  75                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20220313 044934.391 INFO             PET0 index=  76                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20220313 044934.391 INFO             PET0 index=  77                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name
20220313 044934.391 INFO             PET0 index=  78                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20220313 044934.391 INFO             PET0 index=  79                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20220313 044934.391 INFO             PET0 index=  80                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20220313 044934.391 INFO             PET0 index=  81                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20220313 044934.391 INFO             PET0 index=  82                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20220313 044934.391 INFO             PET0 index=  83                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20220313 044934.392 INFO             PET0 index=  84                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET0 index=  85                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET0 index=  86                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20220313 044934.392 INFO             PET0 index=  87                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20220313 044934.392 INFO             PET0 index=  88                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20220313 044934.392 INFO             PET0 index=  89                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20220313 044934.392 INFO             PET0 index=  90               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20220313 044934.392 INFO             PET0 index=  91                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20220313 044934.392 INFO             PET0 index=  92               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20220313 044934.392 INFO             PET0 index=  93                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20220313 044934.392 INFO             PET0 index=  94            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20220313 044934.392 INFO             PET0 index=  95                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20220313 044934.392 INFO             PET0 index=  96                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20220313 044934.392 INFO             PET0 index=  97                                       MPIR_CVAR_CH3_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20220313 044934.392 INFO             PET0 index=  98                              MPIR_CVAR_CH3_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine.
20220313 044934.392 INFO             PET0 index=  99                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20220313 044934.392 INFO             PET0 index= 100                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET0 index= 101                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET0 index= 102                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET0 index= 103                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET0 index= 104           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20220313 044934.392 INFO             PET0 main: --- VMK::log() start -------------------------------------
20220313 044934.392 INFO             PET0 main: vm located at: 0x1d0d730
20220313 044934.392 INFO             PET0 main: petCount=6 localPet=0 mypthid=47753352835904 currentSsiPe=0
20220313 044934.392 INFO             PET0 main: Current system level affinity pinning for local PET:
20220313 044934.392 INFO             PET0 main:  SSIPE=0
20220313 044934.392 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.392 INFO             PET0 main: ssiCount=1 localSsi=0
20220313 044934.392 INFO             PET0 main: mpionly=1 threadsflag=0
20220313 044934.392 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.392 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.392 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20220313 044934.392 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.392 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20220313 044934.392 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.393 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20220313 044934.393 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.393 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20220313 044934.393 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.393 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20220313 044934.393 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.393 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20220313 044934.393 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20220313 044934.393 INFO             PET0 Executing 'userm1_setvm'
20220313 044934.394 INFO             PET0 Executing 'userm1_register'
20220313 044934.394 INFO             PET0 Executing 'userm2_setvm'
20220313 044934.394 INFO             PET0 Executing 'userm2_register'
20220313 044934.397 INFO             PET0 Entering 'user1_run'
20220313 044934.398 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220313 044934.398 INFO             PET0 model1: vm located at: 0x1d5a380
20220313 044934.398 INFO             PET0 model1: petCount=6 localPet=0 mypthid=47753352835904 currentSsiPe=0
20220313 044934.398 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220313 044934.398 INFO             PET0 model1:  SSIPE=0
20220313 044934.398 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.398 INFO             PET0 model1: ssiCount=1 localSsi=0
20220313 044934.398 INFO             PET0 model1: mpionly=1 threadsflag=0
20220313 044934.398 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.398 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.398 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220313 044934.398 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.398 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220313 044934.398 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.398 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220313 044934.398 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.398 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220313 044934.398 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.398 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220313 044934.398 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.398 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220313 044934.398 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220313 044934.398 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044936.076 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044937.648 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044939.219 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044940.791 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044942.362 INFO             PET0 Exiting 'user1_run'
20220313 044942.370 INFO             PET0 Entering 'user2_run'
20220313 044942.370 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220313 044942.370 INFO             PET0 model2: vm located at: 0x1d5d130
20220313 044942.370 INFO             PET0 model2: petCount=6 localPet=0 mypthid=47753352835904 currentSsiPe=0
20220313 044942.370 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220313 044942.370 INFO             PET0 model2:  SSIPE=0
20220313 044942.370 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044942.370 INFO             PET0 model2: ssiCount=1 localSsi=0
20220313 044942.370 INFO             PET0 model2: mpionly=1 threadsflag=0
20220313 044942.370 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044942.370 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044942.370 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220313 044942.370 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044942.370 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220313 044942.370 INFO             PET0 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044942.370 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220313 044942.370 INFO             PET0 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044942.370 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220313 044942.370 INFO             PET0 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044942.370 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220313 044942.370 INFO             PET0 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044942.370 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220313 044942.370 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220313 044942.371 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 044943.707 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 044945.043 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 044946.380 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 044947.716 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 044949.052 INFO             PET0  user2_run: All data correct.
20220313 044949.052 INFO             PET0 Exiting 'user2_run'
20220313 044949.052 INFO             PET0 Entering 'user1_run'
20220313 044949.052 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220313 044949.052 INFO             PET0 model1: vm located at: 0x1d5a380
20220313 044949.052 INFO             PET0 model1: petCount=6 localPet=0 mypthid=47753352835904 currentSsiPe=0
20220313 044949.052 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220313 044949.052 INFO             PET0 model1:  SSIPE=0
20220313 044949.053 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044949.053 INFO             PET0 model1: ssiCount=1 localSsi=0
20220313 044949.053 INFO             PET0 model1: mpionly=1 threadsflag=0
20220313 044949.053 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044949.053 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044949.053 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220313 044949.053 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044949.053 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220313 044949.053 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044949.053 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220313 044949.053 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044949.053 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220313 044949.053 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044949.053 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220313 044949.053 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044949.053 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220313 044949.053 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220313 044949.053 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044950.624 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044952.195 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044953.767 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044955.338 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220313 044956.909 INFO             PET0 Exiting 'user1_run'
20220313 044956.910 INFO             PET0 Entering 'user2_run'
20220313 044956.910 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220313 044956.910 INFO             PET0 model2: vm located at: 0x1d5d130
20220313 044956.910 INFO             PET0 model2: petCount=6 localPet=0 mypthid=47753352835904 currentSsiPe=0
20220313 044956.910 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220313 044956.910 INFO             PET0 model2:  SSIPE=0
20220313 044956.910 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044956.910 INFO             PET0 model2: ssiCount=1 localSsi=0
20220313 044956.910 INFO             PET0 model2: mpionly=1 threadsflag=0
20220313 044956.910 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044956.910 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044956.910 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220313 044956.910 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044956.910 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220313 044956.910 INFO             PET0 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044956.910 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220313 044956.910 INFO             PET0 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044956.910 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220313 044956.910 INFO             PET0 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044956.910 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220313 044956.910 INFO             PET0 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044956.910 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220313 044956.910 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220313 044956.911 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 044958.247 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 044959.583 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 045000.919 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 045002.255 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220313 045003.591 INFO             PET0  user2_run: All data correct.
20220313 045003.591 INFO             PET0 Exiting 'user2_run'
20220313 045003.592 INFO             PET0  NUMBER_OF_PROCESSORS           6
20220313 045003.592 INFO             PET0  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220313 045003.592 INFO             PET0 Finalizing ESMF
20220313 044934.387 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.388 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220313 044934.388 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220313 044934.388 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220313 044934.388 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220313 044934.388 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.388 INFO             PET1 Running with ESMF Version   : v8.3.0b09
20220313 044934.388 INFO             PET1 ESMF library build date/time: "Mar 13 2022" "04:05:42"
20220313 044934.388 INFO             PET1 ESMF library build location : /mnt/lfs4/HFIP/hfv3gfs/Mark.Potts/intel_2020.2_mvapich2_g_develop
20220313 044934.388 INFO             PET1 ESMF_COMM                   : mvapich2
20220313 044934.389 INFO             PET1 ESMF_MOAB                   : enabled
20220313 044934.389 INFO             PET1 ESMF_LAPACK                 : enabled
20220313 044934.389 INFO             PET1 ESMF_NETCDF                 : enabled
20220313 044934.389 INFO             PET1 ESMF_PNETCDF                : disabled
20220313 044934.389 INFO             PET1 ESMF_PIO                    : enabled
20220313 044934.389 INFO             PET1 ESMF_YAMLCPP                : enabled
20220313 044934.389 INFO             PET1 --- VMK::logSystem() start -------------------------------
20220313 044934.389 INFO             PET1 esmfComm=mvapich2
20220313 044934.389 INFO             PET1 isPthreadsEnabled=0
20220313 044934.389 INFO             PET1 isOpenMPEnabled=1
20220313 044934.389 INFO             PET1 isOpenACCEnabled=0
20220313 044934.389 INFO             PET1 isSsiSharedMemoryEnabled=1
20220313 044934.389 INFO             PET1 ssiCount=1 peCount=6
20220313 044934.389 INFO             PET1 PE=0 SSI=0 SSIPE=0
20220313 044934.389 INFO             PET1 PE=1 SSI=0 SSIPE=1
20220313 044934.389 INFO             PET1 PE=2 SSI=0 SSIPE=2
20220313 044934.389 INFO             PET1 PE=3 SSI=0 SSIPE=3
20220313 044934.389 INFO             PET1 PE=4 SSI=0 SSIPE=4
20220313 044934.389 INFO             PET1 PE=5 SSI=0 SSIPE=5
20220313 044934.389 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20220313 044934.389 INFO             PET1 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220313 044934.389 INFO             PET1 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220313 044934.389 INFO             PET1 index=   2                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220313 044934.389 INFO             PET1 index=   3                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.389 INFO             PET1 index=   4                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220313 044934.389 INFO             PET1 index=   5                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220313 044934.389 INFO             PET1 index=   6                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220313 044934.389 INFO             PET1 index=   7                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=   8                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=   9                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20220313 044934.390 INFO             PET1 index=  10                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220313 044934.390 INFO             PET1 index=  11                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=  12                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=  13                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=  14                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=  15                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220313 044934.390 INFO             PET1 index=  16                       MPIR_CVAR_GATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for gather operation.
20220313 044934.390 INFO             PET1 index=  17                    MPIR_CVAR_ALLGATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allgather operation.
20220313 044934.390 INFO             PET1 index=  18                    MPIR_CVAR_ALLREDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allreduce operation.
20220313 044934.390 INFO             PET1 index=  19                     MPIR_CVAR_ALLTOALL_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoall operation.
20220313 044934.390 INFO             PET1 index=  20                    MPIR_CVAR_ALLTOALLV_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoallv operation.
20220313 044934.390 INFO             PET1 index=  21                        MPIR_CVAR_BCAST_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for broadcast operation.
20220313 044934.390 INFO             PET1 index=  22                       MPIR_CVAR_REDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for reduce operation.
20220313 044934.390 INFO             PET1 index=  23                      MPIR_CVAR_SCATTER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for scatter operation.
20220313 044934.390 INFO             PET1 index=  24                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=  25                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220313 044934.390 INFO             PET1 index=  26                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220313 044934.390 INFO             PET1 index=  27                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20220313 044934.390 INFO             PET1 index=  28                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.390 INFO             PET1 index=  29                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220313 044934.390 INFO             PET1 index=  30                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220313 044934.390 INFO             PET1 index=  31                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220313 044934.390 INFO             PET1 index=  32                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20220313 044934.390 INFO             PET1 index=  33                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20220313 044934.390 INFO             PET1 index=  34                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20220313 044934.390 INFO             PET1 index=  35                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPIR_WaitForDebugger-time.
20220313 044934.390 INFO             PET1 index=  36                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20220313 044934.390 INFO             PET1 index=  37                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20220313 044934.390 INFO             PET1 index=  38                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20220313 044934.390 INFO             PET1 index=  39                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20220313 044934.390 INFO             PET1 index=  40                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20220313 044934.391 INFO             PET1 index=  41                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20220313 044934.391 INFO             PET1 index=  42                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20220313 044934.391 INFO             PET1 index=  43                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20220313 044934.391 INFO             PET1 index=  44                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20220313 044934.391 INFO             PET1 index=  45                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20220313 044934.391 INFO             PET1 index=  46                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20220313 044934.391 INFO             PET1 index=  47                                   MPIR_CVAR_FORCE_ARCH_TYPE : This parameter forces the architecture type.
20220313 044934.391 INFO             PET1 index=  48                                    MPIR_CVAR_FORCE_HCA_TYPE : This parameter forces the HCA type.
20220313 044934.391 INFO             PET1 index=  49                                      MPIR_CVAR_USE_BLOCKING : Setting this parameter enables mvapich2 to use blocking mode progress. MPI applications do not take up any CPU when they are waiting for incoming messages.
20220313 044934.391 INFO             PET1 index=  50                                    MPIR_CVAR_USE_SHARED_MEM : Use shared memory for intra-node communication.
20220313 044934.391 INFO             PET1 index=  51                               MPIR_CVAR_ON_DEMAND_THRESHOLD : This defines threshold for enabling on-demand connection management scheme. When the size of the job is larger than the threshold value, on-demand connection management will be used.
20220313 044934.391 INFO             PET1 index=  52                                      MPIR_CVAR_ENABLE_SHARP : This enables the hardware-based SHArP collectives.
20220313 044934.391 INFO             PET1 index=  53                                     MPIR_CVAR_SM_SCHEDULING : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET1 index=  54                     MPIR_CVAR_SMALL_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with small message sizes.
20220313 044934.391 INFO             PET1 index=  55                       MPIR_CVAR_MED_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with medium message sizes.
20220313 044934.391 INFO             PET1 index=  56                               MPIR_CVAR_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET1 index=  57                                         MPIR_CVAR_NUM_PORTS : This specifies the number of ports per InfiniBand adapter to be used for communication per adapter on an end node.
20220313 044934.391 INFO             PET1 index=  58                                   MPIR_CVAR_NUM_QP_PER_PORT : This parameter indicates number of queue pairs per port to be used for communication on an end node. This is useful in the presence of multiple send/recv engines available per port for data transfer.
20220313 044934.391 INFO             PET1 index=  59                               MPIR_CVAR_IBA_EAGER_THRESHOLD : This specifies the switch point between eager and rendezvous protocol in MVAPICH2. For better performance, the value of MPIR_CVAR_MV2_IBA_EAGER_THRESHOLD should be set the same as MPIR_CVAR_MV2_VBUF_TOTAL_SIZE.
20220313 044934.391 INFO             PET1 index=  60                                MPIR_CVAR_STRIPING_THRESHOLD : This parameter specifies the message size above which we begin to stripe the message across multiple rails (if present).
20220313 044934.391 INFO             PET1 index=  61                    MPIR_CVAR_RAIL_SHARING_MED_MSG_THRESHOLD : This specifies the threshold for the medium message size beyond which medium rail sharing striping will take place.
20220313 044934.391 INFO             PET1 index=  62                  MPIR_CVAR_RAIL_SHARING_LARGE_MSG_THRESHOLD : This specifies the threshold for the large message size beyond which large rail sharing striping will be effective.
20220313 044934.391 INFO             PET1 index=  63                                         MPIR_CVAR_USE_MCAST : Set this to 1, to enable hardware multicast support in collective communication.
20220313 044934.391 INFO             PET1 index=  64                                MPIR_CVAR_COALESCE_THRESHOLD : This parameter determines the threshhold for message coalescing.
20220313 044934.391 INFO             PET1 index=  65                                      MPIR_CVAR_USE_COALESCE : Coalesce multiple small messages into a single message to increase small message throughput.
20220313 044934.391 INFO             PET1 index=  66                                     MPIR_CVAR_RNDV_PROTOCOL : The value of this variable can be set to choose different rendezvous protocols. RPUT (default RDMA-Write) RGET (RDMA Read based), R3 (send/recv based).
20220313 044934.391 INFO             PET1 index=  67                                        MPIR_CVAR_SPIN_COUNT : This is the number of the connection manager polls for new control messages from UD channel for each interrupt. This may be increased to reduce the interrupt overhead when many incoming control messages from  UD channel at the same time.
20220313 044934.391 INFO             PET1 index=  68                                       MPIR_CVAR_DEFAULT_MTU : The internal MTU size. For Gen2, this parameter should be a string instead of an integer. Valid values are: IBV_MTU_256, IBV_MTU_512, IBV_MTU_1024, IBV_MTU_2048, IBV_MTU_4096.
20220313 044934.391 INFO             PET1 index=  69                                 MPIR_CVAR_NUM_CQES_PER_POLL : Maximum number of InfiniBand messages retrieved from the completion queue in one attempt.
20220313 044934.391 INFO             PET1 index=  70                                       MPIR_CVAR_USE_RDMA_CM : This parameter enables the use of RDMA CM for establishing the connections.
20220313 044934.391 INFO             PET1 index=  71                                    MPIR_CVAR_USE_IWARP_MODE : This parameter enables the library to run in iWARP mode.
20220313 044934.391 INFO             PET1 index=  72                                       MPIR_CVAR_SUPPORT_DPM : This option enables the dynamic process management interface and on-demand connection management.
20220313 044934.391 INFO             PET1 index=  73                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20220313 044934.391 INFO             PET1 index=  74                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20220313 044934.391 INFO             PET1 index=  75                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20220313 044934.391 INFO             PET1 index=  76                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20220313 044934.391 INFO             PET1 index=  77                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name
20220313 044934.391 INFO             PET1 index=  78                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20220313 044934.391 INFO             PET1 index=  79                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20220313 044934.391 INFO             PET1 index=  80                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20220313 044934.391 INFO             PET1 index=  81                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20220313 044934.392 INFO             PET1 index=  82                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20220313 044934.392 INFO             PET1 index=  83                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20220313 044934.392 INFO             PET1 index=  84                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET1 index=  85                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET1 index=  86                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20220313 044934.392 INFO             PET1 index=  87                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20220313 044934.392 INFO             PET1 index=  88                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20220313 044934.392 INFO             PET1 index=  89                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20220313 044934.392 INFO             PET1 index=  90               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20220313 044934.392 INFO             PET1 index=  91                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20220313 044934.392 INFO             PET1 index=  92               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20220313 044934.392 INFO             PET1 index=  93                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20220313 044934.392 INFO             PET1 index=  94            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20220313 044934.392 INFO             PET1 index=  95                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20220313 044934.392 INFO             PET1 index=  96                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20220313 044934.392 INFO             PET1 index=  97                                       MPIR_CVAR_CH3_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20220313 044934.392 INFO             PET1 index=  98                              MPIR_CVAR_CH3_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine.
20220313 044934.392 INFO             PET1 index=  99                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20220313 044934.392 INFO             PET1 index= 100                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET1 index= 101                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET1 index= 102                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET1 index= 103                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET1 index= 104           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20220313 044934.392 INFO             PET1 main: --- VMK::log() start -------------------------------------
20220313 044934.392 INFO             PET1 main: vm located at: 0x1469730
20220313 044934.392 INFO             PET1 main: petCount=6 localPet=1 mypthid=47979887903552 currentSsiPe=1
20220313 044934.392 INFO             PET1 main: Current system level affinity pinning for local PET:
20220313 044934.392 INFO             PET1 main:  SSIPE=1
20220313 044934.392 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.392 INFO             PET1 main: ssiCount=1 localSsi=0
20220313 044934.393 INFO             PET1 main: mpionly=1 threadsflag=0
20220313 044934.393 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.393 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.393 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20220313 044934.393 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.393 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20220313 044934.393 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.393 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20220313 044934.393 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.393 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20220313 044934.393 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.393 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20220313 044934.393 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.393 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20220313 044934.393 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20220313 044934.394 INFO             PET1 Executing 'userm1_setvm'
20220313 044934.394 INFO             PET1 Executing 'userm1_register'
20220313 044934.394 INFO             PET1 Executing 'userm2_setvm'
20220313 044934.394 INFO             PET1 Executing 'userm2_register'
20220313 044934.397 INFO             PET1 Entering 'user1_run'
20220313 044934.398 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220313 044934.398 INFO             PET1 model1: vm located at: 0x16fe420
20220313 044934.398 INFO             PET1 model1: petCount=6 localPet=1 mypthid=47979887903552 currentSsiPe=1
20220313 044934.398 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220313 044934.398 INFO             PET1 model1:  SSIPE=1
20220313 044934.398 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.398 INFO             PET1 model1: ssiCount=1 localSsi=0
20220313 044934.398 INFO             PET1 model1: mpionly=1 threadsflag=0
20220313 044934.398 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.398 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.398 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220313 044934.398 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.398 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220313 044934.398 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.398 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220313 044934.398 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.398 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220313 044934.398 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.398 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220313 044934.398 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.398 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220313 044934.398 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220313 044934.398 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044936.076 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044937.649 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044939.221 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044940.792 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044942.364 INFO             PET1 Exiting 'user1_run'
20220313 044942.370 INFO             PET1 Entering 'user2_run'
20220313 044942.370 INFO             PET1 model2: --- VMK::log() start -------------------------------------
20220313 044942.370 INFO             PET1 model2: vm located at: 0x179ec30
20220313 044942.370 INFO             PET1 model2: petCount=6 localPet=1 mypthid=47979887903552 currentSsiPe=1
20220313 044942.370 INFO             PET1 model2: Current system level affinity pinning for local PET:
20220313 044942.370 INFO             PET1 model2:  SSIPE=1
20220313 044942.370 INFO             PET1 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044942.370 INFO             PET1 model2: ssiCount=1 localSsi=0
20220313 044942.370 INFO             PET1 model2: mpionly=1 threadsflag=0
20220313 044942.370 INFO             PET1 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044942.370 INFO             PET1 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044942.370 INFO             PET1 model2:  PE=0 SSI=0 SSIPE=0
20220313 044942.370 INFO             PET1 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044942.370 INFO             PET1 model2:  PE=1 SSI=0 SSIPE=1
20220313 044942.370 INFO             PET1 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044942.370 INFO             PET1 model2:  PE=2 SSI=0 SSIPE=2
20220313 044942.370 INFO             PET1 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044942.370 INFO             PET1 model2:  PE=3 SSI=0 SSIPE=3
20220313 044942.370 INFO             PET1 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044942.370 INFO             PET1 model2:  PE=4 SSI=0 SSIPE=4
20220313 044942.370 INFO             PET1 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044942.370 INFO             PET1 model2:  PE=5 SSI=0 SSIPE=5
20220313 044942.370 INFO             PET1 model2: --- VMK::log() end ---------------------------------------
20220313 044942.371 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044943.707 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044945.043 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044946.378 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044947.714 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044949.050 INFO             PET1  user2_run: All data correct.
20220313 044949.050 INFO             PET1 Exiting 'user2_run'
20220313 044949.050 INFO             PET1 Entering 'user1_run'
20220313 044949.051 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220313 044949.051 INFO             PET1 model1: vm located at: 0x16fe420
20220313 044949.051 INFO             PET1 model1: petCount=6 localPet=1 mypthid=47979887903552 currentSsiPe=1
20220313 044949.051 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220313 044949.051 INFO             PET1 model1:  SSIPE=1
20220313 044949.051 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044949.051 INFO             PET1 model1: ssiCount=1 localSsi=0
20220313 044949.051 INFO             PET1 model1: mpionly=1 threadsflag=0
20220313 044949.051 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044949.051 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044949.051 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220313 044949.051 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044949.051 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220313 044949.051 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044949.051 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220313 044949.051 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044949.051 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220313 044949.051 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044949.051 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220313 044949.051 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044949.051 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220313 044949.051 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220313 044949.051 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044950.623 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044952.194 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044953.766 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044955.338 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044956.909 INFO             PET1 Exiting 'user1_run'
20220313 044956.910 INFO             PET1 Entering 'user2_run'
20220313 044956.910 INFO             PET1 model2: --- VMK::log() start -------------------------------------
20220313 044956.910 INFO             PET1 model2: vm located at: 0x179ec30
20220313 044956.910 INFO             PET1 model2: petCount=6 localPet=1 mypthid=47979887903552 currentSsiPe=1
20220313 044956.910 INFO             PET1 model2: Current system level affinity pinning for local PET:
20220313 044956.910 INFO             PET1 model2:  SSIPE=1
20220313 044956.910 INFO             PET1 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044956.910 INFO             PET1 model2: ssiCount=1 localSsi=0
20220313 044956.910 INFO             PET1 model2: mpionly=1 threadsflag=0
20220313 044956.910 INFO             PET1 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044956.910 INFO             PET1 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044956.910 INFO             PET1 model2:  PE=0 SSI=0 SSIPE=0
20220313 044956.910 INFO             PET1 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044956.910 INFO             PET1 model2:  PE=1 SSI=0 SSIPE=1
20220313 044956.910 INFO             PET1 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044956.910 INFO             PET1 model2:  PE=2 SSI=0 SSIPE=2
20220313 044956.910 INFO             PET1 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044956.910 INFO             PET1 model2:  PE=3 SSI=0 SSIPE=3
20220313 044956.910 INFO             PET1 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044956.910 INFO             PET1 model2:  PE=4 SSI=0 SSIPE=4
20220313 044956.910 INFO             PET1 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044956.910 INFO             PET1 model2:  PE=5 SSI=0 SSIPE=5
20220313 044956.910 INFO             PET1 model2: --- VMK::log() end ---------------------------------------
20220313 044956.911 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044958.247 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 044959.583 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 045000.919 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 045002.255 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:            1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220313 045003.591 INFO             PET1  user2_run: All data correct.
20220313 045003.591 INFO             PET1 Exiting 'user2_run'
20220313 045003.592 INFO             PET1  NUMBER_OF_PROCESSORS           6
20220313 045003.592 INFO             PET1  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220313 045003.592 INFO             PET1 Finalizing ESMF
20220313 044934.387 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220313 044934.387 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220313 044934.387 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220313 044934.387 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220313 044934.387 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET2 Running with ESMF Version   : v8.3.0b09
20220313 044934.387 INFO             PET2 ESMF library build date/time: "Mar 13 2022" "04:05:42"
20220313 044934.387 INFO             PET2 ESMF library build location : /mnt/lfs4/HFIP/hfv3gfs/Mark.Potts/intel_2020.2_mvapich2_g_develop
20220313 044934.387 INFO             PET2 ESMF_COMM                   : mvapich2
20220313 044934.388 INFO             PET2 ESMF_MOAB                   : enabled
20220313 044934.388 INFO             PET2 ESMF_LAPACK                 : enabled
20220313 044934.388 INFO             PET2 ESMF_NETCDF                 : enabled
20220313 044934.388 INFO             PET2 ESMF_PNETCDF                : disabled
20220313 044934.388 INFO             PET2 ESMF_PIO                    : enabled
20220313 044934.388 INFO             PET2 ESMF_YAMLCPP                : enabled
20220313 044934.389 INFO             PET2 --- VMK::logSystem() start -------------------------------
20220313 044934.389 INFO             PET2 esmfComm=mvapich2
20220313 044934.389 INFO             PET2 isPthreadsEnabled=0
20220313 044934.389 INFO             PET2 isOpenMPEnabled=1
20220313 044934.389 INFO             PET2 isOpenACCEnabled=0
20220313 044934.389 INFO             PET2 isSsiSharedMemoryEnabled=1
20220313 044934.389 INFO             PET2 ssiCount=1 peCount=6
20220313 044934.389 INFO             PET2 PE=0 SSI=0 SSIPE=0
20220313 044934.389 INFO             PET2 PE=1 SSI=0 SSIPE=1
20220313 044934.389 INFO             PET2 PE=2 SSI=0 SSIPE=2
20220313 044934.389 INFO             PET2 PE=3 SSI=0 SSIPE=3
20220313 044934.389 INFO             PET2 PE=4 SSI=0 SSIPE=4
20220313 044934.389 INFO             PET2 PE=5 SSI=0 SSIPE=5
20220313 044934.389 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20220313 044934.389 INFO             PET2 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220313 044934.389 INFO             PET2 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220313 044934.389 INFO             PET2 index=   2                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220313 044934.389 INFO             PET2 index=   3                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.389 INFO             PET2 index=   4                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220313 044934.389 INFO             PET2 index=   5                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220313 044934.389 INFO             PET2 index=   6                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220313 044934.389 INFO             PET2 index=   7                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=   8                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=   9                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20220313 044934.390 INFO             PET2 index=  10                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220313 044934.390 INFO             PET2 index=  11                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=  12                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=  13                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=  14                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=  15                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220313 044934.390 INFO             PET2 index=  16                       MPIR_CVAR_GATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for gather operation.
20220313 044934.390 INFO             PET2 index=  17                    MPIR_CVAR_ALLGATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allgather operation.
20220313 044934.390 INFO             PET2 index=  18                    MPIR_CVAR_ALLREDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allreduce operation.
20220313 044934.390 INFO             PET2 index=  19                     MPIR_CVAR_ALLTOALL_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoall operation.
20220313 044934.390 INFO             PET2 index=  20                    MPIR_CVAR_ALLTOALLV_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoallv operation.
20220313 044934.390 INFO             PET2 index=  21                        MPIR_CVAR_BCAST_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for broadcast operation.
20220313 044934.390 INFO             PET2 index=  22                       MPIR_CVAR_REDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for reduce operation.
20220313 044934.390 INFO             PET2 index=  23                      MPIR_CVAR_SCATTER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for scatter operation.
20220313 044934.390 INFO             PET2 index=  24                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=  25                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220313 044934.390 INFO             PET2 index=  26                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220313 044934.390 INFO             PET2 index=  27                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20220313 044934.390 INFO             PET2 index=  28                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.390 INFO             PET2 index=  29                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220313 044934.390 INFO             PET2 index=  30                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220313 044934.390 INFO             PET2 index=  31                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220313 044934.390 INFO             PET2 index=  32                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20220313 044934.390 INFO             PET2 index=  33                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20220313 044934.390 INFO             PET2 index=  34                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20220313 044934.390 INFO             PET2 index=  35                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPIR_WaitForDebugger-time.
20220313 044934.390 INFO             PET2 index=  36                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20220313 044934.390 INFO             PET2 index=  37                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20220313 044934.390 INFO             PET2 index=  38                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20220313 044934.390 INFO             PET2 index=  39                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20220313 044934.390 INFO             PET2 index=  40                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20220313 044934.390 INFO             PET2 index=  41                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20220313 044934.390 INFO             PET2 index=  42                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20220313 044934.390 INFO             PET2 index=  43                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20220313 044934.391 INFO             PET2 index=  44                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20220313 044934.391 INFO             PET2 index=  45                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20220313 044934.391 INFO             PET2 index=  46                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20220313 044934.391 INFO             PET2 index=  47                                   MPIR_CVAR_FORCE_ARCH_TYPE : This parameter forces the architecture type.
20220313 044934.391 INFO             PET2 index=  48                                    MPIR_CVAR_FORCE_HCA_TYPE : This parameter forces the HCA type.
20220313 044934.391 INFO             PET2 index=  49                                      MPIR_CVAR_USE_BLOCKING : Setting this parameter enables mvapich2 to use blocking mode progress. MPI applications do not take up any CPU when they are waiting for incoming messages.
20220313 044934.391 INFO             PET2 index=  50                                    MPIR_CVAR_USE_SHARED_MEM : Use shared memory for intra-node communication.
20220313 044934.391 INFO             PET2 index=  51                               MPIR_CVAR_ON_DEMAND_THRESHOLD : This defines threshold for enabling on-demand connection management scheme. When the size of the job is larger than the threshold value, on-demand connection management will be used.
20220313 044934.391 INFO             PET2 index=  52                                      MPIR_CVAR_ENABLE_SHARP : This enables the hardware-based SHArP collectives.
20220313 044934.391 INFO             PET2 index=  53                                     MPIR_CVAR_SM_SCHEDULING : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET2 index=  54                     MPIR_CVAR_SMALL_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with small message sizes.
20220313 044934.391 INFO             PET2 index=  55                       MPIR_CVAR_MED_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with medium message sizes.
20220313 044934.391 INFO             PET2 index=  56                               MPIR_CVAR_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET2 index=  57                                         MPIR_CVAR_NUM_PORTS : This specifies the number of ports per InfiniBand adapter to be used for communication per adapter on an end node.
20220313 044934.391 INFO             PET2 index=  58                                   MPIR_CVAR_NUM_QP_PER_PORT : This parameter indicates number of queue pairs per port to be used for communication on an end node. This is useful in the presence of multiple send/recv engines available per port for data transfer.
20220313 044934.391 INFO             PET2 index=  59                               MPIR_CVAR_IBA_EAGER_THRESHOLD : This specifies the switch point between eager and rendezvous protocol in MVAPICH2. For better performance, the value of MPIR_CVAR_MV2_IBA_EAGER_THRESHOLD should be set the same as MPIR_CVAR_MV2_VBUF_TOTAL_SIZE.
20220313 044934.391 INFO             PET2 index=  60                                MPIR_CVAR_STRIPING_THRESHOLD : This parameter specifies the message size above which we begin to stripe the message across multiple rails (if present).
20220313 044934.391 INFO             PET2 index=  61                    MPIR_CVAR_RAIL_SHARING_MED_MSG_THRESHOLD : This specifies the threshold for the medium message size beyond which medium rail sharing striping will take place.
20220313 044934.391 INFO             PET2 index=  62                  MPIR_CVAR_RAIL_SHARING_LARGE_MSG_THRESHOLD : This specifies the threshold for the large message size beyond which large rail sharing striping will be effective.
20220313 044934.391 INFO             PET2 index=  63                                         MPIR_CVAR_USE_MCAST : Set this to 1, to enable hardware multicast support in collective communication.
20220313 044934.391 INFO             PET2 index=  64                                MPIR_CVAR_COALESCE_THRESHOLD : This parameter determines the threshhold for message coalescing.
20220313 044934.391 INFO             PET2 index=  65                                      MPIR_CVAR_USE_COALESCE : Coalesce multiple small messages into a single message to increase small message throughput.
20220313 044934.391 INFO             PET2 index=  66                                     MPIR_CVAR_RNDV_PROTOCOL : The value of this variable can be set to choose different rendezvous protocols. RPUT (default RDMA-Write) RGET (RDMA Read based), R3 (send/recv based).
20220313 044934.391 INFO             PET2 index=  67                                        MPIR_CVAR_SPIN_COUNT : This is the number of the connection manager polls for new control messages from UD channel for each interrupt. This may be increased to reduce the interrupt overhead when many incoming control messages from  UD channel at the same time.
20220313 044934.391 INFO             PET2 index=  68                                       MPIR_CVAR_DEFAULT_MTU : The internal MTU size. For Gen2, this parameter should be a string instead of an integer. Valid values are: IBV_MTU_256, IBV_MTU_512, IBV_MTU_1024, IBV_MTU_2048, IBV_MTU_4096.
20220313 044934.391 INFO             PET2 index=  69                                 MPIR_CVAR_NUM_CQES_PER_POLL : Maximum number of InfiniBand messages retrieved from the completion queue in one attempt.
20220313 044934.391 INFO             PET2 index=  70                                       MPIR_CVAR_USE_RDMA_CM : This parameter enables the use of RDMA CM for establishing the connections.
20220313 044934.391 INFO             PET2 index=  71                                    MPIR_CVAR_USE_IWARP_MODE : This parameter enables the library to run in iWARP mode.
20220313 044934.391 INFO             PET2 index=  72                                       MPIR_CVAR_SUPPORT_DPM : This option enables the dynamic process management interface and on-demand connection management.
20220313 044934.391 INFO             PET2 index=  73                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20220313 044934.391 INFO             PET2 index=  74                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20220313 044934.391 INFO             PET2 index=  75                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20220313 044934.391 INFO             PET2 index=  76                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20220313 044934.391 INFO             PET2 index=  77                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name
20220313 044934.391 INFO             PET2 index=  78                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20220313 044934.391 INFO             PET2 index=  79                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20220313 044934.391 INFO             PET2 index=  80                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20220313 044934.391 INFO             PET2 index=  81                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20220313 044934.391 INFO             PET2 index=  82                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20220313 044934.391 INFO             PET2 index=  83                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20220313 044934.391 INFO             PET2 index=  84                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20220313 044934.391 INFO             PET2 index=  85                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET2 index=  86                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20220313 044934.392 INFO             PET2 index=  87                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20220313 044934.392 INFO             PET2 index=  88                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20220313 044934.392 INFO             PET2 index=  89                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20220313 044934.392 INFO             PET2 index=  90               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20220313 044934.392 INFO             PET2 index=  91                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20220313 044934.392 INFO             PET2 index=  92               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20220313 044934.392 INFO             PET2 index=  93                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20220313 044934.392 INFO             PET2 index=  94            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20220313 044934.392 INFO             PET2 index=  95                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20220313 044934.392 INFO             PET2 index=  96                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20220313 044934.392 INFO             PET2 index=  97                                       MPIR_CVAR_CH3_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20220313 044934.392 INFO             PET2 index=  98                              MPIR_CVAR_CH3_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine.
20220313 044934.392 INFO             PET2 index=  99                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20220313 044934.392 INFO             PET2 index= 100                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET2 index= 101                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET2 index= 102                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET2 index= 103                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET2 index= 104           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20220313 044934.392 INFO             PET2 main: --- VMK::log() start -------------------------------------
20220313 044934.392 INFO             PET2 main: vm located at: 0x24c4730
20220313 044934.392 INFO             PET2 main: petCount=6 localPet=2 mypthid=47791241253696 currentSsiPe=2
20220313 044934.392 INFO             PET2 main: Current system level affinity pinning for local PET:
20220313 044934.392 INFO             PET2 main:  SSIPE=2
20220313 044934.392 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.392 INFO             PET2 main: ssiCount=1 localSsi=0
20220313 044934.392 INFO             PET2 main: mpionly=1 threadsflag=0
20220313 044934.392 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.392 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.392 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20220313 044934.392 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.392 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20220313 044934.392 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.392 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20220313 044934.392 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.392 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20220313 044934.392 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.392 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20220313 044934.392 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.393 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20220313 044934.393 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20220313 044934.394 INFO             PET2 Executing 'userm1_setvm'
20220313 044934.394 INFO             PET2 Executing 'userm1_register'
20220313 044934.394 INFO             PET2 Executing 'userm2_setvm'
20220313 044934.394 INFO             PET2 Executing 'userm2_register'
20220313 044934.397 INFO             PET2 Entering 'user1_run'
20220313 044934.398 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220313 044934.398 INFO             PET2 model1: vm located at: 0x2759420
20220313 044934.398 INFO             PET2 model1: petCount=6 localPet=2 mypthid=47791241253696 currentSsiPe=2
20220313 044934.398 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220313 044934.398 INFO             PET2 model1:  SSIPE=2
20220313 044934.398 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.398 INFO             PET2 model1: ssiCount=1 localSsi=0
20220313 044934.398 INFO             PET2 model1: mpionly=1 threadsflag=0
20220313 044934.398 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.398 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.398 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220313 044934.398 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.398 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220313 044934.398 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.398 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220313 044934.398 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.398 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220313 044934.398 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.398 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220313 044934.398 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.398 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220313 044934.398 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220313 044934.398 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044936.076 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044937.647 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044939.219 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044940.790 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044942.361 INFO             PET2 Exiting 'user1_run'
20220313 044942.370 INFO             PET2 Entering 'user2_run'
20220313 044942.370 INFO             PET2 model2: --- VMK::log() start -------------------------------------
20220313 044942.370 INFO             PET2 model2: vm located at: 0x27f9c30
20220313 044942.370 INFO             PET2 model2: petCount=6 localPet=2 mypthid=47791241253696 currentSsiPe=2
20220313 044942.370 INFO             PET2 model2: Current system level affinity pinning for local PET:
20220313 044942.370 INFO             PET2 model2:  SSIPE=2
20220313 044942.370 INFO             PET2 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044942.370 INFO             PET2 model2: ssiCount=1 localSsi=0
20220313 044942.370 INFO             PET2 model2: mpionly=1 threadsflag=0
20220313 044942.370 INFO             PET2 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044942.370 INFO             PET2 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044942.370 INFO             PET2 model2:  PE=0 SSI=0 SSIPE=0
20220313 044942.370 INFO             PET2 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044942.370 INFO             PET2 model2:  PE=1 SSI=0 SSIPE=1
20220313 044942.370 INFO             PET2 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044942.370 INFO             PET2 model2:  PE=2 SSI=0 SSIPE=2
20220313 044942.370 INFO             PET2 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044942.370 INFO             PET2 model2:  PE=3 SSI=0 SSIPE=3
20220313 044942.370 INFO             PET2 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044942.370 INFO             PET2 model2:  PE=4 SSI=0 SSIPE=4
20220313 044942.370 INFO             PET2 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044942.370 INFO             PET2 model2:  PE=5 SSI=0 SSIPE=5
20220313 044942.370 INFO             PET2 model2: --- VMK::log() end ---------------------------------------
20220313 044942.371 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044943.707 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044945.042 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044946.378 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044947.714 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044949.050 INFO             PET2  user2_run: All data correct.
20220313 044949.050 INFO             PET2 Exiting 'user2_run'
20220313 044949.050 INFO             PET2 Entering 'user1_run'
20220313 044949.050 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220313 044949.050 INFO             PET2 model1: vm located at: 0x2759420
20220313 044949.050 INFO             PET2 model1: petCount=6 localPet=2 mypthid=47791241253696 currentSsiPe=2
20220313 044949.050 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220313 044949.050 INFO             PET2 model1:  SSIPE=2
20220313 044949.050 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044949.050 INFO             PET2 model1: ssiCount=1 localSsi=0
20220313 044949.050 INFO             PET2 model1: mpionly=1 threadsflag=0
20220313 044949.050 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044949.050 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044949.050 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220313 044949.050 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044949.050 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220313 044949.050 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044949.050 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220313 044949.050 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044949.050 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220313 044949.050 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044949.050 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220313 044949.050 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044949.050 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220313 044949.050 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220313 044949.050 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044950.621 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044952.192 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044953.763 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044955.334 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044956.905 INFO             PET2 Exiting 'user1_run'
20220313 044956.906 INFO             PET2 Entering 'user2_run'
20220313 044956.906 INFO             PET2 model2: --- VMK::log() start -------------------------------------
20220313 044956.906 INFO             PET2 model2: vm located at: 0x27f9c30
20220313 044956.906 INFO             PET2 model2: petCount=6 localPet=2 mypthid=47791241253696 currentSsiPe=2
20220313 044956.906 INFO             PET2 model2: Current system level affinity pinning for local PET:
20220313 044956.906 INFO             PET2 model2:  SSIPE=2
20220313 044956.906 INFO             PET2 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044956.906 INFO             PET2 model2: ssiCount=1 localSsi=0
20220313 044956.906 INFO             PET2 model2: mpionly=1 threadsflag=0
20220313 044956.906 INFO             PET2 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044956.906 INFO             PET2 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044956.906 INFO             PET2 model2:  PE=0 SSI=0 SSIPE=0
20220313 044956.906 INFO             PET2 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044956.906 INFO             PET2 model2:  PE=1 SSI=0 SSIPE=1
20220313 044956.906 INFO             PET2 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044956.906 INFO             PET2 model2:  PE=2 SSI=0 SSIPE=2
20220313 044956.906 INFO             PET2 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044956.906 INFO             PET2 model2:  PE=3 SSI=0 SSIPE=3
20220313 044956.906 INFO             PET2 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044956.906 INFO             PET2 model2:  PE=4 SSI=0 SSIPE=4
20220313 044956.906 INFO             PET2 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044956.906 INFO             PET2 model2:  PE=5 SSI=0 SSIPE=5
20220313 044956.906 INFO             PET2 model2: --- VMK::log() end ---------------------------------------
20220313 044956.907 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044958.242 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 044959.578 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 045000.914 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 045002.250 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:            2  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220313 045003.585 INFO             PET2  user2_run: All data correct.
20220313 045003.585 INFO             PET2 Exiting 'user2_run'
20220313 045003.586 INFO             PET2  NUMBER_OF_PROCESSORS           6
20220313 045003.586 INFO             PET2  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220313 045003.586 INFO             PET2 Finalizing ESMF
20220313 044934.388 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.388 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220313 044934.388 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220313 044934.388 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220313 044934.388 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220313 044934.388 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.388 INFO             PET3 Running with ESMF Version   : v8.3.0b09
20220313 044934.388 INFO             PET3 ESMF library build date/time: "Mar 13 2022" "04:05:42"
20220313 044934.388 INFO             PET3 ESMF library build location : /mnt/lfs4/HFIP/hfv3gfs/Mark.Potts/intel_2020.2_mvapich2_g_develop
20220313 044934.388 INFO             PET3 ESMF_COMM                   : mvapich2
20220313 044934.389 INFO             PET3 ESMF_MOAB                   : enabled
20220313 044934.389 INFO             PET3 ESMF_LAPACK                 : enabled
20220313 044934.389 INFO             PET3 ESMF_NETCDF                 : enabled
20220313 044934.389 INFO             PET3 ESMF_PNETCDF                : disabled
20220313 044934.389 INFO             PET3 ESMF_PIO                    : enabled
20220313 044934.389 INFO             PET3 ESMF_YAMLCPP                : enabled
20220313 044934.389 INFO             PET3 --- VMK::logSystem() start -------------------------------
20220313 044934.389 INFO             PET3 esmfComm=mvapich2
20220313 044934.389 INFO             PET3 isPthreadsEnabled=0
20220313 044934.389 INFO             PET3 isOpenMPEnabled=1
20220313 044934.389 INFO             PET3 isOpenACCEnabled=0
20220313 044934.389 INFO             PET3 isSsiSharedMemoryEnabled=1
20220313 044934.389 INFO             PET3 ssiCount=1 peCount=6
20220313 044934.389 INFO             PET3 PE=0 SSI=0 SSIPE=0
20220313 044934.389 INFO             PET3 PE=1 SSI=0 SSIPE=1
20220313 044934.389 INFO             PET3 PE=2 SSI=0 SSIPE=2
20220313 044934.389 INFO             PET3 PE=3 SSI=0 SSIPE=3
20220313 044934.389 INFO             PET3 PE=4 SSI=0 SSIPE=4
20220313 044934.389 INFO             PET3 PE=5 SSI=0 SSIPE=5
20220313 044934.389 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20220313 044934.389 INFO             PET3 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220313 044934.389 INFO             PET3 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220313 044934.389 INFO             PET3 index=   2                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220313 044934.389 INFO             PET3 index=   3                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.389 INFO             PET3 index=   4                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220313 044934.389 INFO             PET3 index=   5                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220313 044934.389 INFO             PET3 index=   6                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220313 044934.389 INFO             PET3 index=   7                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=   8                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=   9                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20220313 044934.390 INFO             PET3 index=  10                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220313 044934.390 INFO             PET3 index=  11                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=  12                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=  13                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=  14                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=  15                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220313 044934.390 INFO             PET3 index=  16                       MPIR_CVAR_GATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for gather operation.
20220313 044934.390 INFO             PET3 index=  17                    MPIR_CVAR_ALLGATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allgather operation.
20220313 044934.390 INFO             PET3 index=  18                    MPIR_CVAR_ALLREDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allreduce operation.
20220313 044934.390 INFO             PET3 index=  19                     MPIR_CVAR_ALLTOALL_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoall operation.
20220313 044934.390 INFO             PET3 index=  20                    MPIR_CVAR_ALLTOALLV_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoallv operation.
20220313 044934.390 INFO             PET3 index=  21                        MPIR_CVAR_BCAST_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for broadcast operation.
20220313 044934.390 INFO             PET3 index=  22                       MPIR_CVAR_REDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for reduce operation.
20220313 044934.390 INFO             PET3 index=  23                      MPIR_CVAR_SCATTER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for scatter operation.
20220313 044934.390 INFO             PET3 index=  24                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=  25                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220313 044934.390 INFO             PET3 index=  26                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220313 044934.390 INFO             PET3 index=  27                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20220313 044934.390 INFO             PET3 index=  28                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.390 INFO             PET3 index=  29                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220313 044934.390 INFO             PET3 index=  30                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220313 044934.390 INFO             PET3 index=  31                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220313 044934.390 INFO             PET3 index=  32                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20220313 044934.390 INFO             PET3 index=  33                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20220313 044934.390 INFO             PET3 index=  34                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20220313 044934.390 INFO             PET3 index=  35                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPIR_WaitForDebugger-time.
20220313 044934.390 INFO             PET3 index=  36                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20220313 044934.390 INFO             PET3 index=  37                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20220313 044934.390 INFO             PET3 index=  38                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20220313 044934.390 INFO             PET3 index=  39                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20220313 044934.390 INFO             PET3 index=  40                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20220313 044934.390 INFO             PET3 index=  41                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20220313 044934.390 INFO             PET3 index=  42                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20220313 044934.390 INFO             PET3 index=  43                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20220313 044934.391 INFO             PET3 index=  44                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20220313 044934.391 INFO             PET3 index=  45                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20220313 044934.391 INFO             PET3 index=  46                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20220313 044934.391 INFO             PET3 index=  47                                   MPIR_CVAR_FORCE_ARCH_TYPE : This parameter forces the architecture type.
20220313 044934.391 INFO             PET3 index=  48                                    MPIR_CVAR_FORCE_HCA_TYPE : This parameter forces the HCA type.
20220313 044934.391 INFO             PET3 index=  49                                      MPIR_CVAR_USE_BLOCKING : Setting this parameter enables mvapich2 to use blocking mode progress. MPI applications do not take up any CPU when they are waiting for incoming messages.
20220313 044934.391 INFO             PET3 index=  50                                    MPIR_CVAR_USE_SHARED_MEM : Use shared memory for intra-node communication.
20220313 044934.391 INFO             PET3 index=  51                               MPIR_CVAR_ON_DEMAND_THRESHOLD : This defines threshold for enabling on-demand connection management scheme. When the size of the job is larger than the threshold value, on-demand connection management will be used.
20220313 044934.391 INFO             PET3 index=  52                                      MPIR_CVAR_ENABLE_SHARP : This enables the hardware-based SHArP collectives.
20220313 044934.391 INFO             PET3 index=  53                                     MPIR_CVAR_SM_SCHEDULING : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET3 index=  54                     MPIR_CVAR_SMALL_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with small message sizes.
20220313 044934.391 INFO             PET3 index=  55                       MPIR_CVAR_MED_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with medium message sizes.
20220313 044934.391 INFO             PET3 index=  56                               MPIR_CVAR_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET3 index=  57                                         MPIR_CVAR_NUM_PORTS : This specifies the number of ports per InfiniBand adapter to be used for communication per adapter on an end node.
20220313 044934.391 INFO             PET3 index=  58                                   MPIR_CVAR_NUM_QP_PER_PORT : This parameter indicates number of queue pairs per port to be used for communication on an end node. This is useful in the presence of multiple send/recv engines available per port for data transfer.
20220313 044934.391 INFO             PET3 index=  59                               MPIR_CVAR_IBA_EAGER_THRESHOLD : This specifies the switch point between eager and rendezvous protocol in MVAPICH2. For better performance, the value of MPIR_CVAR_MV2_IBA_EAGER_THRESHOLD should be set the same as MPIR_CVAR_MV2_VBUF_TOTAL_SIZE.
20220313 044934.391 INFO             PET3 index=  60                                MPIR_CVAR_STRIPING_THRESHOLD : This parameter specifies the message size above which we begin to stripe the message across multiple rails (if present).
20220313 044934.391 INFO             PET3 index=  61                    MPIR_CVAR_RAIL_SHARING_MED_MSG_THRESHOLD : This specifies the threshold for the medium message size beyond which medium rail sharing striping will take place.
20220313 044934.391 INFO             PET3 index=  62                  MPIR_CVAR_RAIL_SHARING_LARGE_MSG_THRESHOLD : This specifies the threshold for the large message size beyond which large rail sharing striping will be effective.
20220313 044934.391 INFO             PET3 index=  63                                         MPIR_CVAR_USE_MCAST : Set this to 1, to enable hardware multicast support in collective communication.
20220313 044934.391 INFO             PET3 index=  64                                MPIR_CVAR_COALESCE_THRESHOLD : This parameter determines the threshhold for message coalescing.
20220313 044934.391 INFO             PET3 index=  65                                      MPIR_CVAR_USE_COALESCE : Coalesce multiple small messages into a single message to increase small message throughput.
20220313 044934.391 INFO             PET3 index=  66                                     MPIR_CVAR_RNDV_PROTOCOL : The value of this variable can be set to choose different rendezvous protocols. RPUT (default RDMA-Write) RGET (RDMA Read based), R3 (send/recv based).
20220313 044934.391 INFO             PET3 index=  67                                        MPIR_CVAR_SPIN_COUNT : This is the number of the connection manager polls for new control messages from UD channel for each interrupt. This may be increased to reduce the interrupt overhead when many incoming control messages from  UD channel at the same time.
20220313 044934.391 INFO             PET3 index=  68                                       MPIR_CVAR_DEFAULT_MTU : The internal MTU size. For Gen2, this parameter should be a string instead of an integer. Valid values are: IBV_MTU_256, IBV_MTU_512, IBV_MTU_1024, IBV_MTU_2048, IBV_MTU_4096.
20220313 044934.391 INFO             PET3 index=  69                                 MPIR_CVAR_NUM_CQES_PER_POLL : Maximum number of InfiniBand messages retrieved from the completion queue in one attempt.
20220313 044934.391 INFO             PET3 index=  70                                       MPIR_CVAR_USE_RDMA_CM : This parameter enables the use of RDMA CM for establishing the connections.
20220313 044934.391 INFO             PET3 index=  71                                    MPIR_CVAR_USE_IWARP_MODE : This parameter enables the library to run in iWARP mode.
20220313 044934.391 INFO             PET3 index=  72                                       MPIR_CVAR_SUPPORT_DPM : This option enables the dynamic process management interface and on-demand connection management.
20220313 044934.391 INFO             PET3 index=  73                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20220313 044934.391 INFO             PET3 index=  74                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20220313 044934.391 INFO             PET3 index=  75                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20220313 044934.391 INFO             PET3 index=  76                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20220313 044934.391 INFO             PET3 index=  77                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name
20220313 044934.391 INFO             PET3 index=  78                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20220313 044934.391 INFO             PET3 index=  79                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20220313 044934.391 INFO             PET3 index=  80                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20220313 044934.391 INFO             PET3 index=  81                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20220313 044934.391 INFO             PET3 index=  82                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20220313 044934.391 INFO             PET3 index=  83                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20220313 044934.391 INFO             PET3 index=  84                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20220313 044934.391 INFO             PET3 index=  85                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET3 index=  86                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20220313 044934.392 INFO             PET3 index=  87                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20220313 044934.392 INFO             PET3 index=  88                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20220313 044934.392 INFO             PET3 index=  89                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20220313 044934.392 INFO             PET3 index=  90               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20220313 044934.392 INFO             PET3 index=  91                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20220313 044934.392 INFO             PET3 index=  92               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20220313 044934.392 INFO             PET3 index=  93                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20220313 044934.392 INFO             PET3 index=  94            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20220313 044934.392 INFO             PET3 index=  95                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20220313 044934.392 INFO             PET3 index=  96                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20220313 044934.392 INFO             PET3 index=  97                                       MPIR_CVAR_CH3_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20220313 044934.392 INFO             PET3 index=  98                              MPIR_CVAR_CH3_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine.
20220313 044934.392 INFO             PET3 index=  99                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20220313 044934.392 INFO             PET3 index= 100                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET3 index= 101                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET3 index= 102                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET3 index= 103                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET3 index= 104           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20220313 044934.392 INFO             PET3 main: --- VMK::log() start -------------------------------------
20220313 044934.392 INFO             PET3 main: vm located at: 0x1d24730
20220313 044934.392 INFO             PET3 main: petCount=6 localPet=3 mypthid=47391016833856 currentSsiPe=3
20220313 044934.392 INFO             PET3 main: Current system level affinity pinning for local PET:
20220313 044934.392 INFO             PET3 main:  SSIPE=3
20220313 044934.392 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.392 INFO             PET3 main: ssiCount=1 localSsi=0
20220313 044934.392 INFO             PET3 main: mpionly=1 threadsflag=0
20220313 044934.392 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.392 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.392 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20220313 044934.392 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.392 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20220313 044934.392 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.392 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20220313 044934.392 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.392 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20220313 044934.392 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.392 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20220313 044934.393 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.393 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20220313 044934.393 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20220313 044934.394 INFO             PET3 Executing 'userm1_setvm'
20220313 044934.394 INFO             PET3 Executing 'userm1_register'
20220313 044934.394 INFO             PET3 Executing 'userm2_setvm'
20220313 044934.394 INFO             PET3 Executing 'userm2_register'
20220313 044934.397 INFO             PET3 Entering 'user1_run'
20220313 044934.398 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220313 044934.398 INFO             PET3 model1: vm located at: 0x1fb9420
20220313 044934.398 INFO             PET3 model1: petCount=6 localPet=3 mypthid=47391016833856 currentSsiPe=3
20220313 044934.398 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220313 044934.398 INFO             PET3 model1:  SSIPE=3
20220313 044934.398 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.398 INFO             PET3 model1: ssiCount=1 localSsi=0
20220313 044934.398 INFO             PET3 model1: mpionly=1 threadsflag=0
20220313 044934.398 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.398 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.398 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220313 044934.398 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.398 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220313 044934.398 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.398 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220313 044934.398 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.398 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220313 044934.398 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.398 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220313 044934.398 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.398 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220313 044934.398 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220313 044934.398 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044936.076 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044937.647 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044939.219 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044940.790 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044942.361 INFO             PET3 Exiting 'user1_run'
20220313 044942.370 INFO             PET3 Entering 'user2_run'
20220313 044942.370 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220313 044942.370 INFO             PET3 model2: vm located at: 0x2059c30
20220313 044942.370 INFO             PET3 model2: petCount=6 localPet=3 mypthid=47391016833856 currentSsiPe=3
20220313 044942.370 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220313 044942.370 INFO             PET3 model2:  SSIPE=3
20220313 044942.370 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044942.370 INFO             PET3 model2: ssiCount=1 localSsi=0
20220313 044942.370 INFO             PET3 model2: mpionly=1 threadsflag=0
20220313 044942.370 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044942.370 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044942.370 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220313 044942.370 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044942.370 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220313 044942.370 INFO             PET3 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044942.370 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220313 044942.370 INFO             PET3 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044942.370 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220313 044942.370 INFO             PET3 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044942.370 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220313 044942.370 INFO             PET3 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044942.370 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220313 044942.370 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220313 044942.371 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044943.707 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044945.043 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044946.378 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044947.714 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044949.050 INFO             PET3  user2_run: All data correct.
20220313 044949.050 INFO             PET3 Exiting 'user2_run'
20220313 044949.050 INFO             PET3 Entering 'user1_run'
20220313 044949.051 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220313 044949.051 INFO             PET3 model1: vm located at: 0x1fb9420
20220313 044949.051 INFO             PET3 model1: petCount=6 localPet=3 mypthid=47391016833856 currentSsiPe=3
20220313 044949.051 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220313 044949.051 INFO             PET3 model1:  SSIPE=3
20220313 044949.051 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044949.051 INFO             PET3 model1: ssiCount=1 localSsi=0
20220313 044949.051 INFO             PET3 model1: mpionly=1 threadsflag=0
20220313 044949.051 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044949.051 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044949.051 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220313 044949.051 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044949.051 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220313 044949.051 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044949.051 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220313 044949.051 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044949.051 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220313 044949.051 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044949.051 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220313 044949.051 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044949.051 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220313 044949.051 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220313 044949.051 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044950.622 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044952.193 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044953.764 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044955.335 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044956.906 INFO             PET3 Exiting 'user1_run'
20220313 044956.906 INFO             PET3 Entering 'user2_run'
20220313 044956.906 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220313 044956.906 INFO             PET3 model2: vm located at: 0x2059c30
20220313 044956.906 INFO             PET3 model2: petCount=6 localPet=3 mypthid=47391016833856 currentSsiPe=3
20220313 044956.907 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220313 044956.907 INFO             PET3 model2:  SSIPE=3
20220313 044956.907 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044956.907 INFO             PET3 model2: ssiCount=1 localSsi=0
20220313 044956.907 INFO             PET3 model2: mpionly=1 threadsflag=0
20220313 044956.907 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044956.907 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044956.907 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220313 044956.907 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044956.907 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220313 044956.907 INFO             PET3 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044956.907 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220313 044956.907 INFO             PET3 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044956.907 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220313 044956.907 INFO             PET3 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044956.907 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220313 044956.907 INFO             PET3 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044956.907 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220313 044956.907 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220313 044956.907 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044958.243 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 044959.579 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 045000.914 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 045002.251 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220313 045003.586 INFO             PET3  user2_run: All data correct.
20220313 045003.586 INFO             PET3 Exiting 'user2_run'
20220313 045003.587 INFO             PET3  NUMBER_OF_PROCESSORS           6
20220313 045003.587 INFO             PET3  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220313 045003.587 INFO             PET3 Finalizing ESMF
20220313 044934.387 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220313 044934.387 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220313 044934.387 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220313 044934.387 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220313 044934.387 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET4 Running with ESMF Version   : v8.3.0b09
20220313 044934.387 INFO             PET4 ESMF library build date/time: "Mar 13 2022" "04:05:42"
20220313 044934.387 INFO             PET4 ESMF library build location : /mnt/lfs4/HFIP/hfv3gfs/Mark.Potts/intel_2020.2_mvapich2_g_develop
20220313 044934.387 INFO             PET4 ESMF_COMM                   : mvapich2
20220313 044934.388 INFO             PET4 ESMF_MOAB                   : enabled
20220313 044934.388 INFO             PET4 ESMF_LAPACK                 : enabled
20220313 044934.388 INFO             PET4 ESMF_NETCDF                 : enabled
20220313 044934.388 INFO             PET4 ESMF_PNETCDF                : disabled
20220313 044934.388 INFO             PET4 ESMF_PIO                    : enabled
20220313 044934.388 INFO             PET4 ESMF_YAMLCPP                : enabled
20220313 044934.389 INFO             PET4 --- VMK::logSystem() start -------------------------------
20220313 044934.389 INFO             PET4 esmfComm=mvapich2
20220313 044934.389 INFO             PET4 isPthreadsEnabled=0
20220313 044934.389 INFO             PET4 isOpenMPEnabled=1
20220313 044934.389 INFO             PET4 isOpenACCEnabled=0
20220313 044934.389 INFO             PET4 isSsiSharedMemoryEnabled=1
20220313 044934.389 INFO             PET4 ssiCount=1 peCount=6
20220313 044934.389 INFO             PET4 PE=0 SSI=0 SSIPE=0
20220313 044934.389 INFO             PET4 PE=1 SSI=0 SSIPE=1
20220313 044934.389 INFO             PET4 PE=2 SSI=0 SSIPE=2
20220313 044934.389 INFO             PET4 PE=3 SSI=0 SSIPE=3
20220313 044934.389 INFO             PET4 PE=4 SSI=0 SSIPE=4
20220313 044934.389 INFO             PET4 PE=5 SSI=0 SSIPE=5
20220313 044934.389 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20220313 044934.389 INFO             PET4 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220313 044934.389 INFO             PET4 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220313 044934.389 INFO             PET4 index=   2                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220313 044934.389 INFO             PET4 index=   3                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.389 INFO             PET4 index=   4                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220313 044934.389 INFO             PET4 index=   5                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220313 044934.389 INFO             PET4 index=   6                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220313 044934.389 INFO             PET4 index=   7                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=   8                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=   9                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20220313 044934.390 INFO             PET4 index=  10                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220313 044934.390 INFO             PET4 index=  11                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=  12                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=  13                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=  14                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=  15                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220313 044934.390 INFO             PET4 index=  16                       MPIR_CVAR_GATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for gather operation.
20220313 044934.390 INFO             PET4 index=  17                    MPIR_CVAR_ALLGATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allgather operation.
20220313 044934.390 INFO             PET4 index=  18                    MPIR_CVAR_ALLREDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allreduce operation.
20220313 044934.390 INFO             PET4 index=  19                     MPIR_CVAR_ALLTOALL_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoall operation.
20220313 044934.390 INFO             PET4 index=  20                    MPIR_CVAR_ALLTOALLV_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoallv operation.
20220313 044934.390 INFO             PET4 index=  21                        MPIR_CVAR_BCAST_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for broadcast operation.
20220313 044934.390 INFO             PET4 index=  22                       MPIR_CVAR_REDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for reduce operation.
20220313 044934.390 INFO             PET4 index=  23                      MPIR_CVAR_SCATTER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for scatter operation.
20220313 044934.390 INFO             PET4 index=  24                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=  25                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220313 044934.390 INFO             PET4 index=  26                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220313 044934.390 INFO             PET4 index=  27                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20220313 044934.390 INFO             PET4 index=  28                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.390 INFO             PET4 index=  29                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220313 044934.390 INFO             PET4 index=  30                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220313 044934.390 INFO             PET4 index=  31                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220313 044934.390 INFO             PET4 index=  32                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20220313 044934.390 INFO             PET4 index=  33                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20220313 044934.390 INFO             PET4 index=  34                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20220313 044934.390 INFO             PET4 index=  35                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPIR_WaitForDebugger-time.
20220313 044934.390 INFO             PET4 index=  36                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20220313 044934.390 INFO             PET4 index=  37                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20220313 044934.390 INFO             PET4 index=  38                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20220313 044934.390 INFO             PET4 index=  39                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20220313 044934.390 INFO             PET4 index=  40                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20220313 044934.390 INFO             PET4 index=  41                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20220313 044934.390 INFO             PET4 index=  42                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20220313 044934.390 INFO             PET4 index=  43                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20220313 044934.391 INFO             PET4 index=  44                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20220313 044934.391 INFO             PET4 index=  45                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20220313 044934.391 INFO             PET4 index=  46                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20220313 044934.391 INFO             PET4 index=  47                                   MPIR_CVAR_FORCE_ARCH_TYPE : This parameter forces the architecture type.
20220313 044934.391 INFO             PET4 index=  48                                    MPIR_CVAR_FORCE_HCA_TYPE : This parameter forces the HCA type.
20220313 044934.391 INFO             PET4 index=  49                                      MPIR_CVAR_USE_BLOCKING : Setting this parameter enables mvapich2 to use blocking mode progress. MPI applications do not take up any CPU when they are waiting for incoming messages.
20220313 044934.391 INFO             PET4 index=  50                                    MPIR_CVAR_USE_SHARED_MEM : Use shared memory for intra-node communication.
20220313 044934.391 INFO             PET4 index=  51                               MPIR_CVAR_ON_DEMAND_THRESHOLD : This defines threshold for enabling on-demand connection management scheme. When the size of the job is larger than the threshold value, on-demand connection management will be used.
20220313 044934.391 INFO             PET4 index=  52                                      MPIR_CVAR_ENABLE_SHARP : This enables the hardware-based SHArP collectives.
20220313 044934.391 INFO             PET4 index=  53                                     MPIR_CVAR_SM_SCHEDULING : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET4 index=  54                     MPIR_CVAR_SMALL_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with small message sizes.
20220313 044934.391 INFO             PET4 index=  55                       MPIR_CVAR_MED_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with medium message sizes.
20220313 044934.391 INFO             PET4 index=  56                               MPIR_CVAR_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET4 index=  57                                         MPIR_CVAR_NUM_PORTS : This specifies the number of ports per InfiniBand adapter to be used for communication per adapter on an end node.
20220313 044934.391 INFO             PET4 index=  58                                   MPIR_CVAR_NUM_QP_PER_PORT : This parameter indicates number of queue pairs per port to be used for communication on an end node. This is useful in the presence of multiple send/recv engines available per port for data transfer.
20220313 044934.391 INFO             PET4 index=  59                               MPIR_CVAR_IBA_EAGER_THRESHOLD : This specifies the switch point between eager and rendezvous protocol in MVAPICH2. For better performance, the value of MPIR_CVAR_MV2_IBA_EAGER_THRESHOLD should be set the same as MPIR_CVAR_MV2_VBUF_TOTAL_SIZE.
20220313 044934.391 INFO             PET4 index=  60                                MPIR_CVAR_STRIPING_THRESHOLD : This parameter specifies the message size above which we begin to stripe the message across multiple rails (if present).
20220313 044934.391 INFO             PET4 index=  61                    MPIR_CVAR_RAIL_SHARING_MED_MSG_THRESHOLD : This specifies the threshold for the medium message size beyond which medium rail sharing striping will take place.
20220313 044934.391 INFO             PET4 index=  62                  MPIR_CVAR_RAIL_SHARING_LARGE_MSG_THRESHOLD : This specifies the threshold for the large message size beyond which large rail sharing striping will be effective.
20220313 044934.391 INFO             PET4 index=  63                                         MPIR_CVAR_USE_MCAST : Set this to 1, to enable hardware multicast support in collective communication.
20220313 044934.391 INFO             PET4 index=  64                                MPIR_CVAR_COALESCE_THRESHOLD : This parameter determines the threshhold for message coalescing.
20220313 044934.391 INFO             PET4 index=  65                                      MPIR_CVAR_USE_COALESCE : Coalesce multiple small messages into a single message to increase small message throughput.
20220313 044934.391 INFO             PET4 index=  66                                     MPIR_CVAR_RNDV_PROTOCOL : The value of this variable can be set to choose different rendezvous protocols. RPUT (default RDMA-Write) RGET (RDMA Read based), R3 (send/recv based).
20220313 044934.391 INFO             PET4 index=  67                                        MPIR_CVAR_SPIN_COUNT : This is the number of the connection manager polls for new control messages from UD channel for each interrupt. This may be increased to reduce the interrupt overhead when many incoming control messages from  UD channel at the same time.
20220313 044934.391 INFO             PET4 index=  68                                       MPIR_CVAR_DEFAULT_MTU : The internal MTU size. For Gen2, this parameter should be a string instead of an integer. Valid values are: IBV_MTU_256, IBV_MTU_512, IBV_MTU_1024, IBV_MTU_2048, IBV_MTU_4096.
20220313 044934.391 INFO             PET4 index=  69                                 MPIR_CVAR_NUM_CQES_PER_POLL : Maximum number of InfiniBand messages retrieved from the completion queue in one attempt.
20220313 044934.391 INFO             PET4 index=  70                                       MPIR_CVAR_USE_RDMA_CM : This parameter enables the use of RDMA CM for establishing the connections.
20220313 044934.391 INFO             PET4 index=  71                                    MPIR_CVAR_USE_IWARP_MODE : This parameter enables the library to run in iWARP mode.
20220313 044934.391 INFO             PET4 index=  72                                       MPIR_CVAR_SUPPORT_DPM : This option enables the dynamic process management interface and on-demand connection management.
20220313 044934.391 INFO             PET4 index=  73                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20220313 044934.391 INFO             PET4 index=  74                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20220313 044934.391 INFO             PET4 index=  75                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20220313 044934.391 INFO             PET4 index=  76                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20220313 044934.391 INFO             PET4 index=  77                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name
20220313 044934.391 INFO             PET4 index=  78                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20220313 044934.391 INFO             PET4 index=  79                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20220313 044934.391 INFO             PET4 index=  80                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20220313 044934.391 INFO             PET4 index=  81                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20220313 044934.391 INFO             PET4 index=  82                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20220313 044934.391 INFO             PET4 index=  83                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20220313 044934.391 INFO             PET4 index=  84                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET4 index=  85                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET4 index=  86                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20220313 044934.392 INFO             PET4 index=  87                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20220313 044934.392 INFO             PET4 index=  88                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20220313 044934.392 INFO             PET4 index=  89                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20220313 044934.392 INFO             PET4 index=  90               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20220313 044934.392 INFO             PET4 index=  91                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20220313 044934.392 INFO             PET4 index=  92               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20220313 044934.392 INFO             PET4 index=  93                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20220313 044934.392 INFO             PET4 index=  94            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20220313 044934.392 INFO             PET4 index=  95                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20220313 044934.392 INFO             PET4 index=  96                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20220313 044934.392 INFO             PET4 index=  97                                       MPIR_CVAR_CH3_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20220313 044934.392 INFO             PET4 index=  98                              MPIR_CVAR_CH3_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine.
20220313 044934.392 INFO             PET4 index=  99                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20220313 044934.392 INFO             PET4 index= 100                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET4 index= 101                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET4 index= 102                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET4 index= 103                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET4 index= 104           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20220313 044934.392 INFO             PET4 main: --- VMK::log() start -------------------------------------
20220313 044934.392 INFO             PET4 main: vm located at: 0x9ca730
20220313 044934.392 INFO             PET4 main: petCount=6 localPet=4 mypthid=47829962146624 currentSsiPe=4
20220313 044934.392 INFO             PET4 main: Current system level affinity pinning for local PET:
20220313 044934.392 INFO             PET4 main:  SSIPE=4
20220313 044934.392 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.392 INFO             PET4 main: ssiCount=1 localSsi=0
20220313 044934.392 INFO             PET4 main: mpionly=1 threadsflag=0
20220313 044934.392 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.392 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.392 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20220313 044934.392 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.392 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20220313 044934.392 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.392 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20220313 044934.392 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.393 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20220313 044934.393 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.393 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20220313 044934.393 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.393 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20220313 044934.393 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20220313 044934.394 INFO             PET4 Executing 'userm1_setvm'
20220313 044934.394 INFO             PET4 Executing 'userm1_register'
20220313 044934.394 INFO             PET4 Executing 'userm2_setvm'
20220313 044934.394 INFO             PET4 Executing 'userm2_register'
20220313 044934.397 INFO             PET4 Entering 'user1_run'
20220313 044934.398 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220313 044934.398 INFO             PET4 model1: vm located at: 0xc5f420
20220313 044934.398 INFO             PET4 model1: petCount=6 localPet=4 mypthid=47829962146624 currentSsiPe=4
20220313 044934.398 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220313 044934.398 INFO             PET4 model1:  SSIPE=4
20220313 044934.398 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.398 INFO             PET4 model1: ssiCount=1 localSsi=0
20220313 044934.398 INFO             PET4 model1: mpionly=1 threadsflag=0
20220313 044934.398 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.398 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.398 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220313 044934.398 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.398 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220313 044934.398 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.398 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220313 044934.398 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.398 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220313 044934.398 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.398 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220313 044934.398 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.398 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220313 044934.398 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220313 044934.398 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044936.075 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044937.645 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044939.215 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044940.785 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044942.355 INFO             PET4 Exiting 'user1_run'
20220313 044942.370 INFO             PET4 Entering 'user2_run'
20220313 044942.370 INFO             PET4 model2: --- VMK::log() start -------------------------------------
20220313 044942.370 INFO             PET4 model2: vm located at: 0xcffc30
20220313 044942.370 INFO             PET4 model2: petCount=6 localPet=4 mypthid=47829962146624 currentSsiPe=4
20220313 044942.370 INFO             PET4 model2: Current system level affinity pinning for local PET:
20220313 044942.370 INFO             PET4 model2:  SSIPE=4
20220313 044942.370 INFO             PET4 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044942.370 INFO             PET4 model2: ssiCount=1 localSsi=0
20220313 044942.370 INFO             PET4 model2: mpionly=1 threadsflag=0
20220313 044942.370 INFO             PET4 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044942.370 INFO             PET4 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044942.370 INFO             PET4 model2:  PE=0 SSI=0 SSIPE=0
20220313 044942.370 INFO             PET4 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044942.370 INFO             PET4 model2:  PE=1 SSI=0 SSIPE=1
20220313 044942.370 INFO             PET4 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044942.370 INFO             PET4 model2:  PE=2 SSI=0 SSIPE=2
20220313 044942.370 INFO             PET4 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044942.370 INFO             PET4 model2:  PE=3 SSI=0 SSIPE=3
20220313 044942.370 INFO             PET4 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044942.370 INFO             PET4 model2:  PE=4 SSI=0 SSIPE=4
20220313 044942.370 INFO             PET4 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044942.370 INFO             PET4 model2:  PE=5 SSI=0 SSIPE=5
20220313 044942.370 INFO             PET4 model2: --- VMK::log() end ---------------------------------------
20220313 044942.371 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044943.706 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044945.041 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044946.376 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044947.711 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044949.046 INFO             PET4  user2_run: All data correct.
20220313 044949.047 INFO             PET4 Exiting 'user2_run'
20220313 044949.047 INFO             PET4 Entering 'user1_run'
20220313 044949.047 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220313 044949.047 INFO             PET4 model1: vm located at: 0xc5f420
20220313 044949.047 INFO             PET4 model1: petCount=6 localPet=4 mypthid=47829962146624 currentSsiPe=4
20220313 044949.047 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220313 044949.047 INFO             PET4 model1:  SSIPE=4
20220313 044949.047 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044949.047 INFO             PET4 model1: ssiCount=1 localSsi=0
20220313 044949.047 INFO             PET4 model1: mpionly=1 threadsflag=0
20220313 044949.047 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044949.047 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044949.047 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220313 044949.047 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044949.047 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220313 044949.047 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044949.047 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220313 044949.047 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044949.047 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220313 044949.047 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044949.047 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220313 044949.047 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044949.047 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220313 044949.047 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220313 044949.047 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044950.617 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044952.187 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044953.757 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044955.327 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044956.897 INFO             PET4 Exiting 'user1_run'
20220313 044956.898 INFO             PET4 Entering 'user2_run'
20220313 044956.898 INFO             PET4 model2: --- VMK::log() start -------------------------------------
20220313 044956.898 INFO             PET4 model2: vm located at: 0xcffc30
20220313 044956.898 INFO             PET4 model2: petCount=6 localPet=4 mypthid=47829962146624 currentSsiPe=4
20220313 044956.898 INFO             PET4 model2: Current system level affinity pinning for local PET:
20220313 044956.898 INFO             PET4 model2:  SSIPE=4
20220313 044956.898 INFO             PET4 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044956.898 INFO             PET4 model2: ssiCount=1 localSsi=0
20220313 044956.898 INFO             PET4 model2: mpionly=1 threadsflag=0
20220313 044956.898 INFO             PET4 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044956.898 INFO             PET4 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044956.898 INFO             PET4 model2:  PE=0 SSI=0 SSIPE=0
20220313 044956.898 INFO             PET4 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044956.898 INFO             PET4 model2:  PE=1 SSI=0 SSIPE=1
20220313 044956.898 INFO             PET4 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044956.898 INFO             PET4 model2:  PE=2 SSI=0 SSIPE=2
20220313 044956.898 INFO             PET4 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044956.898 INFO             PET4 model2:  PE=3 SSI=0 SSIPE=3
20220313 044956.898 INFO             PET4 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044956.898 INFO             PET4 model2:  PE=4 SSI=0 SSIPE=4
20220313 044956.898 INFO             PET4 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044956.898 INFO             PET4 model2:  PE=5 SSI=0 SSIPE=5
20220313 044956.898 INFO             PET4 model2: --- VMK::log() end ---------------------------------------
20220313 044956.899 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044958.234 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 044959.569 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 045000.904 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 045002.238 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:            4  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220313 045003.573 INFO             PET4  user2_run: All data correct.
20220313 045003.573 INFO             PET4 Exiting 'user2_run'
20220313 045003.574 INFO             PET4  NUMBER_OF_PROCESSORS           6
20220313 045003.574 INFO             PET4  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220313 045003.574 INFO             PET4 Finalizing ESMF
20220313 044934.387 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220313 044934.387 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220313 044934.387 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220313 044934.387 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220313 044934.387 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220313 044934.387 INFO             PET5 Running with ESMF Version   : v8.3.0b09
20220313 044934.387 INFO             PET5 ESMF library build date/time: "Mar 13 2022" "04:05:42"
20220313 044934.387 INFO             PET5 ESMF library build location : /mnt/lfs4/HFIP/hfv3gfs/Mark.Potts/intel_2020.2_mvapich2_g_develop
20220313 044934.387 INFO             PET5 ESMF_COMM                   : mvapich2
20220313 044934.388 INFO             PET5 ESMF_MOAB                   : enabled
20220313 044934.388 INFO             PET5 ESMF_LAPACK                 : enabled
20220313 044934.388 INFO             PET5 ESMF_NETCDF                 : enabled
20220313 044934.388 INFO             PET5 ESMF_PNETCDF                : disabled
20220313 044934.388 INFO             PET5 ESMF_PIO                    : enabled
20220313 044934.388 INFO             PET5 ESMF_YAMLCPP                : enabled
20220313 044934.389 INFO             PET5 --- VMK::logSystem() start -------------------------------
20220313 044934.389 INFO             PET5 esmfComm=mvapich2
20220313 044934.389 INFO             PET5 isPthreadsEnabled=0
20220313 044934.389 INFO             PET5 isOpenMPEnabled=1
20220313 044934.389 INFO             PET5 isOpenACCEnabled=0
20220313 044934.389 INFO             PET5 isSsiSharedMemoryEnabled=1
20220313 044934.389 INFO             PET5 ssiCount=1 peCount=6
20220313 044934.389 INFO             PET5 PE=0 SSI=0 SSIPE=0
20220313 044934.389 INFO             PET5 PE=1 SSI=0 SSIPE=1
20220313 044934.389 INFO             PET5 PE=2 SSI=0 SSIPE=2
20220313 044934.389 INFO             PET5 PE=3 SSI=0 SSIPE=3
20220313 044934.389 INFO             PET5 PE=4 SSI=0 SSIPE=4
20220313 044934.389 INFO             PET5 PE=5 SSI=0 SSIPE=5
20220313 044934.389 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20220313 044934.389 INFO             PET5 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220313 044934.389 INFO             PET5 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220313 044934.389 INFO             PET5 index=   2                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220313 044934.389 INFO             PET5 index=   3                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.389 INFO             PET5 index=   4                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220313 044934.389 INFO             PET5 index=   5                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220313 044934.389 INFO             PET5 index=   6                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220313 044934.390 INFO             PET5 index=   7                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=   8                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=   9                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20220313 044934.390 INFO             PET5 index=  10                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220313 044934.390 INFO             PET5 index=  11                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=  12                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=  13                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=  14                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=  15                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220313 044934.390 INFO             PET5 index=  16                       MPIR_CVAR_GATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for gather operation.
20220313 044934.390 INFO             PET5 index=  17                    MPIR_CVAR_ALLGATHER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allgather operation.
20220313 044934.390 INFO             PET5 index=  18                    MPIR_CVAR_ALLREDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for allreduce operation.
20220313 044934.390 INFO             PET5 index=  19                     MPIR_CVAR_ALLTOALL_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoall operation.
20220313 044934.390 INFO             PET5 index=  20                    MPIR_CVAR_ALLTOALLV_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for alltoallv operation.
20220313 044934.390 INFO             PET5 index=  21                        MPIR_CVAR_BCAST_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for broadcast operation.
20220313 044934.390 INFO             PET5 index=  22                       MPIR_CVAR_REDUCE_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for reduce operation.
20220313 044934.390 INFO             PET5 index=  23                      MPIR_CVAR_SCATTER_COLLECTIVE_ALGORITHM : This CVAR selects proper collective algorithm for scatter operation.
20220313 044934.390 INFO             PET5 index=  24                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=  25                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220313 044934.390 INFO             PET5 index=  26                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220313 044934.390 INFO             PET5 index=  27                 MPIR_CVAR_REDSCAT_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20220313 044934.390 INFO             PET5 index=  28                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220313 044934.390 INFO             PET5 index=  29                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220313 044934.390 INFO             PET5 index=  30                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220313 044934.390 INFO             PET5 index=  31                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220313 044934.390 INFO             PET5 index=  32                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20220313 044934.390 INFO             PET5 index=  33                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20220313 044934.390 INFO             PET5 index=  34                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20220313 044934.390 INFO             PET5 index=  35                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPIR_WaitForDebugger-time.
20220313 044934.390 INFO             PET5 index=  36                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20220313 044934.390 INFO             PET5 index=  37                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20220313 044934.390 INFO             PET5 index=  38                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20220313 044934.390 INFO             PET5 index=  39                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20220313 044934.390 INFO             PET5 index=  40                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20220313 044934.391 INFO             PET5 index=  41                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20220313 044934.391 INFO             PET5 index=  42                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20220313 044934.391 INFO             PET5 index=  43                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20220313 044934.391 INFO             PET5 index=  44                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20220313 044934.391 INFO             PET5 index=  45                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20220313 044934.391 INFO             PET5 index=  46                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20220313 044934.391 INFO             PET5 index=  47                                   MPIR_CVAR_FORCE_ARCH_TYPE : This parameter forces the architecture type.
20220313 044934.391 INFO             PET5 index=  48                                    MPIR_CVAR_FORCE_HCA_TYPE : This parameter forces the HCA type.
20220313 044934.391 INFO             PET5 index=  49                                      MPIR_CVAR_USE_BLOCKING : Setting this parameter enables mvapich2 to use blocking mode progress. MPI applications do not take up any CPU when they are waiting for incoming messages.
20220313 044934.391 INFO             PET5 index=  50                                    MPIR_CVAR_USE_SHARED_MEM : Use shared memory for intra-node communication.
20220313 044934.391 INFO             PET5 index=  51                               MPIR_CVAR_ON_DEMAND_THRESHOLD : This defines threshold for enabling on-demand connection management scheme. When the size of the job is larger than the threshold value, on-demand connection management will be used.
20220313 044934.391 INFO             PET5 index=  52                                      MPIR_CVAR_ENABLE_SHARP : This enables the hardware-based SHArP collectives.
20220313 044934.391 INFO             PET5 index=  53                                     MPIR_CVAR_SM_SCHEDULING : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET5 index=  54                     MPIR_CVAR_SMALL_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with small message sizes.
20220313 044934.391 INFO             PET5 index=  55                       MPIR_CVAR_MED_MSG_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes with medium message sizes.
20220313 044934.391 INFO             PET5 index=  56                               MPIR_CVAR_RAIL_SHARING_POLICY : This specifies the policy that will be used to assign HCAs to each of the processes.
20220313 044934.391 INFO             PET5 index=  57                                         MPIR_CVAR_NUM_PORTS : This specifies the number of ports per InfiniBand adapter to be used for communication per adapter on an end node.
20220313 044934.391 INFO             PET5 index=  58                                   MPIR_CVAR_NUM_QP_PER_PORT : This parameter indicates number of queue pairs per port to be used for communication on an end node. This is useful in the presence of multiple send/recv engines available per port for data transfer.
20220313 044934.391 INFO             PET5 index=  59                               MPIR_CVAR_IBA_EAGER_THRESHOLD : This specifies the switch point between eager and rendezvous protocol in MVAPICH2. For better performance, the value of MPIR_CVAR_MV2_IBA_EAGER_THRESHOLD should be set the same as MPIR_CVAR_MV2_VBUF_TOTAL_SIZE.
20220313 044934.391 INFO             PET5 index=  60                                MPIR_CVAR_STRIPING_THRESHOLD : This parameter specifies the message size above which we begin to stripe the message across multiple rails (if present).
20220313 044934.391 INFO             PET5 index=  61                    MPIR_CVAR_RAIL_SHARING_MED_MSG_THRESHOLD : This specifies the threshold for the medium message size beyond which medium rail sharing striping will take place.
20220313 044934.391 INFO             PET5 index=  62                  MPIR_CVAR_RAIL_SHARING_LARGE_MSG_THRESHOLD : This specifies the threshold for the large message size beyond which large rail sharing striping will be effective.
20220313 044934.391 INFO             PET5 index=  63                                         MPIR_CVAR_USE_MCAST : Set this to 1, to enable hardware multicast support in collective communication.
20220313 044934.391 INFO             PET5 index=  64                                MPIR_CVAR_COALESCE_THRESHOLD : This parameter determines the threshhold for message coalescing.
20220313 044934.391 INFO             PET5 index=  65                                      MPIR_CVAR_USE_COALESCE : Coalesce multiple small messages into a single message to increase small message throughput.
20220313 044934.391 INFO             PET5 index=  66                                     MPIR_CVAR_RNDV_PROTOCOL : The value of this variable can be set to choose different rendezvous protocols. RPUT (default RDMA-Write) RGET (RDMA Read based), R3 (send/recv based).
20220313 044934.391 INFO             PET5 index=  67                                        MPIR_CVAR_SPIN_COUNT : This is the number of the connection manager polls for new control messages from UD channel for each interrupt. This may be increased to reduce the interrupt overhead when many incoming control messages from  UD channel at the same time.
20220313 044934.391 INFO             PET5 index=  68                                       MPIR_CVAR_DEFAULT_MTU : The internal MTU size. For Gen2, this parameter should be a string instead of an integer. Valid values are: IBV_MTU_256, IBV_MTU_512, IBV_MTU_1024, IBV_MTU_2048, IBV_MTU_4096.
20220313 044934.391 INFO             PET5 index=  69                                 MPIR_CVAR_NUM_CQES_PER_POLL : Maximum number of InfiniBand messages retrieved from the completion queue in one attempt.
20220313 044934.391 INFO             PET5 index=  70                                       MPIR_CVAR_USE_RDMA_CM : This parameter enables the use of RDMA CM for establishing the connections.
20220313 044934.391 INFO             PET5 index=  71                                    MPIR_CVAR_USE_IWARP_MODE : This parameter enables the library to run in iWARP mode.
20220313 044934.391 INFO             PET5 index=  72                                       MPIR_CVAR_SUPPORT_DPM : This option enables the dynamic process management interface and on-demand connection management.
20220313 044934.391 INFO             PET5 index=  73                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20220313 044934.391 INFO             PET5 index=  74                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20220313 044934.391 INFO             PET5 index=  75                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20220313 044934.391 INFO             PET5 index=  76                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20220313 044934.391 INFO             PET5 index=  77                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name
20220313 044934.391 INFO             PET5 index=  78                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20220313 044934.391 INFO             PET5 index=  79                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20220313 044934.391 INFO             PET5 index=  80                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20220313 044934.391 INFO             PET5 index=  81                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20220313 044934.392 INFO             PET5 index=  82                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20220313 044934.392 INFO             PET5 index=  83                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20220313 044934.392 INFO             PET5 index=  84                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET5 index=  85                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20220313 044934.392 INFO             PET5 index=  86                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20220313 044934.392 INFO             PET5 index=  87                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20220313 044934.392 INFO             PET5 index=  88                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20220313 044934.392 INFO             PET5 index=  89                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20220313 044934.392 INFO             PET5 index=  90               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20220313 044934.392 INFO             PET5 index=  91                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20220313 044934.392 INFO             PET5 index=  92               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20220313 044934.392 INFO             PET5 index=  93                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20220313 044934.392 INFO             PET5 index=  94            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20220313 044934.392 INFO             PET5 index=  95                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20220313 044934.392 INFO             PET5 index=  96                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20220313 044934.392 INFO             PET5 index=  97                                       MPIR_CVAR_CH3_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20220313 044934.392 INFO             PET5 index=  98                              MPIR_CVAR_CH3_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine.
20220313 044934.392 INFO             PET5 index=  99                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20220313 044934.392 INFO             PET5 index= 100                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET5 index= 101                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET5 index= 102                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20220313 044934.392 INFO             PET5 index= 103                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET5 index= 104           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20220313 044934.392 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20220313 044934.392 INFO             PET5 main: --- VMK::log() start -------------------------------------
20220313 044934.392 INFO             PET5 main: vm located at: 0x203a730
20220313 044934.392 INFO             PET5 main: petCount=6 localPet=5 mypthid=47683535192896 currentSsiPe=5
20220313 044934.392 INFO             PET5 main: Current system level affinity pinning for local PET:
20220313 044934.392 INFO             PET5 main:  SSIPE=5
20220313 044934.392 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.392 INFO             PET5 main: ssiCount=1 localSsi=0
20220313 044934.392 INFO             PET5 main: mpionly=1 threadsflag=0
20220313 044934.393 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.393 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.393 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20220313 044934.393 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.393 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20220313 044934.393 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.393 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20220313 044934.393 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.393 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20220313 044934.393 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.393 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20220313 044934.393 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.393 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20220313 044934.393 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20220313 044934.394 INFO             PET5 Executing 'userm1_setvm'
20220313 044934.394 INFO             PET5 Executing 'userm1_register'
20220313 044934.394 INFO             PET5 Executing 'userm2_setvm'
20220313 044934.394 INFO             PET5 Executing 'userm2_register'
20220313 044934.397 INFO             PET5 Entering 'user1_run'
20220313 044934.398 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220313 044934.398 INFO             PET5 model1: vm located at: 0x22cf420
20220313 044934.398 INFO             PET5 model1: petCount=6 localPet=5 mypthid=47683535192896 currentSsiPe=5
20220313 044934.398 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220313 044934.398 INFO             PET5 model1:  SSIPE=5
20220313 044934.398 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044934.398 INFO             PET5 model1: ssiCount=1 localSsi=0
20220313 044934.398 INFO             PET5 model1: mpionly=1 threadsflag=0
20220313 044934.398 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044934.398 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044934.398 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220313 044934.398 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044934.398 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220313 044934.398 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044934.398 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220313 044934.398 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044934.398 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220313 044934.398 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044934.398 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220313 044934.398 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044934.398 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220313 044934.398 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220313 044934.398 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044936.076 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044937.646 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044939.216 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044940.786 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044942.356 INFO             PET5 Exiting 'user1_run'
20220313 044942.370 INFO             PET5 Entering 'user2_run'
20220313 044942.370 INFO             PET5 model2: --- VMK::log() start -------------------------------------
20220313 044942.370 INFO             PET5 model2: vm located at: 0x236fc30
20220313 044942.370 INFO             PET5 model2: petCount=6 localPet=5 mypthid=47683535192896 currentSsiPe=5
20220313 044942.370 INFO             PET5 model2: Current system level affinity pinning for local PET:
20220313 044942.370 INFO             PET5 model2:  SSIPE=5
20220313 044942.370 INFO             PET5 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044942.370 INFO             PET5 model2: ssiCount=1 localSsi=0
20220313 044942.370 INFO             PET5 model2: mpionly=1 threadsflag=0
20220313 044942.370 INFO             PET5 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044942.370 INFO             PET5 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044942.370 INFO             PET5 model2:  PE=0 SSI=0 SSIPE=0
20220313 044942.370 INFO             PET5 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044942.370 INFO             PET5 model2:  PE=1 SSI=0 SSIPE=1
20220313 044942.370 INFO             PET5 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044942.370 INFO             PET5 model2:  PE=2 SSI=0 SSIPE=2
20220313 044942.370 INFO             PET5 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044942.370 INFO             PET5 model2:  PE=3 SSI=0 SSIPE=3
20220313 044942.370 INFO             PET5 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044942.370 INFO             PET5 model2:  PE=4 SSI=0 SSIPE=4
20220313 044942.370 INFO             PET5 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044942.370 INFO             PET5 model2:  PE=5 SSI=0 SSIPE=5
20220313 044942.370 INFO             PET5 model2: --- VMK::log() end ---------------------------------------
20220313 044942.371 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044943.706 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044945.041 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044946.376 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044947.711 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044949.046 INFO             PET5  user2_run: All data correct.
20220313 044949.047 INFO             PET5 Exiting 'user2_run'
20220313 044949.047 INFO             PET5 Entering 'user1_run'
20220313 044949.047 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220313 044949.047 INFO             PET5 model1: vm located at: 0x22cf420
20220313 044949.047 INFO             PET5 model1: petCount=6 localPet=5 mypthid=47683535192896 currentSsiPe=5
20220313 044949.047 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220313 044949.047 INFO             PET5 model1:  SSIPE=5
20220313 044949.047 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044949.047 INFO             PET5 model1: ssiCount=1 localSsi=0
20220313 044949.047 INFO             PET5 model1: mpionly=1 threadsflag=0
20220313 044949.047 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044949.047 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044949.047 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220313 044949.047 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044949.047 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220313 044949.047 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044949.047 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220313 044949.047 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044949.047 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220313 044949.047 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044949.047 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220313 044949.047 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044949.047 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220313 044949.047 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220313 044949.047 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044950.617 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044952.187 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044953.757 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044955.327 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044956.897 INFO             PET5 Exiting 'user1_run'
20220313 044956.898 INFO             PET5 Entering 'user2_run'
20220313 044956.898 INFO             PET5 model2: --- VMK::log() start -------------------------------------
20220313 044956.898 INFO             PET5 model2: vm located at: 0x236fc30
20220313 044956.898 INFO             PET5 model2: petCount=6 localPet=5 mypthid=47683535192896 currentSsiPe=5
20220313 044956.898 INFO             PET5 model2: Current system level affinity pinning for local PET:
20220313 044956.898 INFO             PET5 model2:  SSIPE=5
20220313 044956.898 INFO             PET5 model2: Current system level OMP_NUM_THREADS setting for local PET: 1
20220313 044956.898 INFO             PET5 model2: ssiCount=1 localSsi=0
20220313 044956.898 INFO             PET5 model2: mpionly=1 threadsflag=0
20220313 044956.898 INFO             PET5 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220313 044956.898 INFO             PET5 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220313 044956.898 INFO             PET5 model2:  PE=0 SSI=0 SSIPE=0
20220313 044956.898 INFO             PET5 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220313 044956.898 INFO             PET5 model2:  PE=1 SSI=0 SSIPE=1
20220313 044956.898 INFO             PET5 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220313 044956.898 INFO             PET5 model2:  PE=2 SSI=0 SSIPE=2
20220313 044956.898 INFO             PET5 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220313 044956.898 INFO             PET5 model2:  PE=3 SSI=0 SSIPE=3
20220313 044956.898 INFO             PET5 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220313 044956.898 INFO             PET5 model2:  PE=4 SSI=0 SSIPE=4
20220313 044956.898 INFO             PET5 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220313 044956.898 INFO             PET5 model2:  PE=5 SSI=0 SSIPE=5
20220313 044956.898 INFO             PET5 model2: --- VMK::log() end ---------------------------------------
20220313 044956.899 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044958.234 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 044959.569 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 045000.904 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 045002.239 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:            5  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220313 045003.574 INFO             PET5  user2_run: All data correct.
20220313 045003.574 INFO             PET5 Exiting 'user2_run'
20220313 045003.574 INFO             PET5  NUMBER_OF_PROCESSORS           6
20220313 045003.574 INFO             PET5  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220313 045003.574 INFO             PET5 Finalizing ESMF
